{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import os\n",
    "import shutil\n",
    "import re\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage\n",
    "from skimage.io import imread, imsave\n",
    "from skimage.transform import resize\n",
    "from skimage import img_as_float32, img_as_ubyte\n",
    "import pickle\n",
    "import cv2\n",
    "\n",
    "from albumentations import (\n",
    "    Compose, HorizontalFlip, ShiftScaleRotate, ElasticTransform,\n",
    "    RandomBrightness, RandomContrast, RandomGamma\n",
    ")\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AugmentationSequence(keras.utils.Sequence):\n",
    "  def __init__(self, x_set, y_set, batch_size, augmentations):\n",
    "    self.x, self.y = x_set, y_set\n",
    "    self.batch_size = batch_size\n",
    "    self.augment = augmentations\n",
    "\n",
    "  def __len__(self):\n",
    "    return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "    batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "    \n",
    "    aug_x = np.zeros(batch_x.shape)\n",
    "    aug_y = np.zeros(batch_y.shape)\n",
    "    \n",
    "    for idx in range(batch_x.shape[0]):\n",
    "      aug = self.augment(image = batch_x[idx,:,:,:], mask = batch_y[idx,:,:,:])\n",
    "      aug_x[idx,:,:,:] = aug[\"image\"]\n",
    "      aug_y[idx,:,:,:] = aug[\"mask\"]\n",
    "    \n",
    "    return aug_x, aug_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144\n"
     ]
    }
   ],
   "source": [
    "img_size = 400\n",
    "#file_pi = open('input/train_dataset.pkl', 'rb') \n",
    "#train_generator =  pickle.load(file_pi)\n",
    "\n",
    "X_train = np.load('input/X_train.npy')\n",
    "Y_train = np.load('input/Y_train.npy')\n",
    "\n",
    "X_val = np.load('input/X_val.npy')\n",
    "Y_val = np.load('input/Y_val.npy')\n",
    "\n",
    "with open('input/epochs.txt', 'r') as file:\n",
    "    steps_per_epoch = file.read().rstrip()\n",
    "\n",
    "steps_per_epoch = 2* int(steps_per_epoch)\n",
    "\n",
    "print(steps_per_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOSS Functions\n",
    "def jaccard_distance_loss(y_true, y_pred, smooth = 100):\n",
    "    intersection = keras.backend.sum(keras.backend.abs(y_true * y_pred), axis = -1)\n",
    "    union = keras.backend.sum(keras.backend.abs(y_true) + keras.backend.abs(y_pred), axis = -1)\n",
    "    jac = (intersection + smooth) / (union - intersection + smooth)\n",
    "    loss = (1 - jac) * smooth\n",
    "    return loss\n",
    "\n",
    "def dice_coef(y_true, y_pred, smooth = 1):\n",
    "    intersection = keras.backend.sum(keras.backend.abs(y_true * y_pred), axis = -1)\n",
    "    union = keras.backend.sum(keras.backend.abs(y_true), -1) + keras.backend.sum(keras.backend.abs(y_pred), -1)\n",
    "    return (2. * intersection + smooth) / (union + smooth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet_model():\n",
    "  \n",
    "  input_img = keras.layers.Input((img_size, img_size, 1), name = \"img\")\n",
    "  \n",
    "  # Contract #1\n",
    "  c1 = keras.layers.Conv2D(16, (3, 3), kernel_initializer = \"he_uniform\", padding = \"same\")(input_img)\n",
    "  c1 = keras.layers.BatchNormalization()(c1)\n",
    "  c1 = keras.layers.Activation(\"relu\")(c1)\n",
    "  c1 = keras.layers.Dropout(0.1)(c1)\n",
    "  c1 = keras.layers.Conv2D(16, (3, 3), kernel_initializer = \"he_uniform\", padding = \"same\")(c1)\n",
    "  c1 = keras.layers.BatchNormalization()(c1)\n",
    "  c1 = keras.layers.Activation(\"relu\")(c1)\n",
    "  p1 = keras.layers.MaxPooling2D((2, 2))(c1)\n",
    "  \n",
    "  # Contract #2\n",
    "  c2 = keras.layers.Conv2D(32, (3, 3), kernel_initializer = \"he_uniform\", padding = \"same\")(p1)\n",
    "  c2 = keras.layers.BatchNormalization()(c2)\n",
    "  c2 = keras.layers.Activation(\"relu\")(c2)\n",
    "  c2 = keras.layers.Dropout(0.2)(c2)\n",
    "  c2 = keras.layers.Conv2D(32, (3, 3), kernel_initializer = \"he_uniform\", padding = \"same\")(c2)\n",
    "  c2 = keras.layers.BatchNormalization()(c2)\n",
    "  c2 = keras.layers.Activation(\"relu\")(c2)\n",
    "  p2 = keras.layers.MaxPooling2D((2, 2))(c2)\n",
    "  \n",
    "  # Contract #3\n",
    "  c3 = keras.layers.Conv2D(64, (3, 3), kernel_initializer = \"he_uniform\", padding = \"same\")(p2)\n",
    "  c3 = keras.layers.BatchNormalization()(c3)\n",
    "  c3 = keras.layers.Activation(\"relu\")(c3)\n",
    "  c3 = keras.layers.Dropout(0.3)(c3)\n",
    "  c3 = keras.layers.Conv2D(64, (3, 3), kernel_initializer = \"he_uniform\", padding = \"same\")(c3)\n",
    "  c3 = keras.layers.BatchNormalization()(c3)\n",
    "  c3 = keras.layers.Activation(\"relu\")(c3)\n",
    "  p3 = keras.layers.MaxPooling2D((2, 2))(c3)\n",
    "  \n",
    "  # Contract #4\n",
    "  c4 = keras.layers.Conv2D(128, (3, 3), kernel_initializer = \"he_uniform\", padding = \"same\")(p3)\n",
    "  c4 = keras.layers.BatchNormalization()(c4)\n",
    "  c4 = keras.layers.Activation(\"relu\")(c4)\n",
    "  c4 = keras.layers.Dropout(0.4)(c4)\n",
    "  c4 = keras.layers.Conv2D(128, (3, 3), kernel_initializer = \"he_uniform\", padding = \"same\")(c4)\n",
    "  c4 = keras.layers.BatchNormalization()(c4)\n",
    "  c4 = keras.layers.Activation(\"relu\")(c4)\n",
    "  p4 = keras.layers.MaxPooling2D((2, 2))(c4)\n",
    "  \n",
    "  # Middle\n",
    "  c5 = keras.layers.Conv2D(256, (3, 3), kernel_initializer = \"he_uniform\", padding = \"same\")(p4)\n",
    "  c5 = keras.layers.BatchNormalization()(c5)\n",
    "  c5 = keras.layers.Activation(\"relu\")(c5)\n",
    "  c5 = keras.layers.Dropout(0.5)(c5)\n",
    "  c5 = keras.layers.Conv2D(256, (3, 3), kernel_initializer = \"he_uniform\", padding = \"same\")(c5)\n",
    "  c5 = keras.layers.BatchNormalization()(c5)\n",
    "  c5 = keras.layers.Activation(\"relu\")(c5)\n",
    "  \n",
    "  # Expand (upscale) #1\n",
    "  u6 = keras.layers.Conv2DTranspose(128, (3, 3), strides = (2, 2), padding = \"same\")(c5)\n",
    "  u6 = keras.layers.concatenate([u6, c4])\n",
    "  c6 = keras.layers.Conv2D(128, (3, 3), kernel_initializer = \"he_uniform\", padding = \"same\")(u6)\n",
    "  c6 = keras.layers.BatchNormalization()(c6)\n",
    "  c6 = keras.layers.Activation(\"relu\")(c6)\n",
    "  c6 = keras.layers.Dropout(0.5)(c6)\n",
    "  c6 = keras.layers.Conv2D(128, (3, 3), kernel_initializer = \"he_uniform\", padding = \"same\")(c6)\n",
    "  c6 = keras.layers.BatchNormalization()(c6)\n",
    "  c6 = keras.layers.Activation(\"relu\")(c6)\n",
    "  \n",
    "  # Expand (upscale) #2\n",
    "  u7 = keras.layers.Conv2DTranspose(64, (3, 3), strides = (2, 2), padding = \"same\")(c6)\n",
    "  u7 = keras.layers.concatenate([u7, c3])\n",
    "  c7 = keras.layers.Conv2D(64, (3, 3), kernel_initializer = \"he_uniform\", padding = \"same\")(u7)\n",
    "  c7 = keras.layers.BatchNormalization()(c7)\n",
    "  c7 = keras.layers.Activation(\"relu\")(c7)\n",
    "  c7 = keras.layers.Dropout(0.5)(c7)\n",
    "  c7 = keras.layers.Conv2D(64, (3, 3), kernel_initializer = \"he_uniform\", padding = \"same\")(c7)\n",
    "  c7 = keras.layers.BatchNormalization()(c7)\n",
    "  c7 = keras.layers.Activation(\"relu\")(c7)\n",
    "  \n",
    "  # Expand (upscale) #3\n",
    "  u8 = keras.layers.Conv2DTranspose(32, (3, 3), strides = (2, 2), padding = \"same\")(c7)\n",
    "  u8 = keras.layers.concatenate([u8, c2])\n",
    "  c8 = keras.layers.Conv2D(32, (3, 3), kernel_initializer = \"he_uniform\", padding = \"same\")(u8)\n",
    "  c8 = keras.layers.BatchNormalization()(c8)\n",
    "  c8 = keras.layers.Activation(\"relu\")(c8)\n",
    "  c8 = keras.layers.Dropout(0.5)(c8)\n",
    "  c8 = keras.layers.Conv2D(32, (3, 3), kernel_initializer = \"he_uniform\", padding = \"same\")(c8)\n",
    "  c8 = keras.layers.BatchNormalization()(c8)\n",
    "  c8 = keras.layers.Activation(\"relu\")(c8)\n",
    "  \n",
    "  # Expand (upscale) #4\n",
    "  u9 = keras.layers.Conv2DTranspose(16, (3, 3), strides = (2, 2), padding = \"same\")(c8)\n",
    "  u9 = keras.layers.concatenate([u9, c1])\n",
    "  c9 = keras.layers.Conv2D(16, (3, 3), kernel_initializer = \"he_uniform\", padding = \"same\")(u9)\n",
    "  c9 = keras.layers.BatchNormalization()(c9)\n",
    "  c9 = keras.layers.Activation(\"relu\")(c9)\n",
    "  c9 = keras.layers.Dropout(0.5)(c9)\n",
    "  c9 = keras.layers.Conv2D(16, (3, 3), kernel_initializer = \"he_uniform\", padding = \"same\")(c9)\n",
    "  c9 = keras.layers.BatchNormalization()(c9)\n",
    "  c9 = keras.layers.Activation(\"relu\")(c9)\n",
    "  \n",
    "  output = keras.layers.Conv2D(1, (1, 1), activation = \"sigmoid\")(c9)\n",
    "  model = keras.Model(inputs = [input_img], outputs = [output])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning rate + Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_learning_rate = keras.callbacks.ReduceLROnPlateau(\n",
    "  monitor = \"val_loss\", \n",
    "  factor = 0.5, \n",
    "  patience = 3, \n",
    "  verbose = 1\n",
    ")\n",
    "\n",
    "#reduce_learning_rate = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=1e-7, verbose=1)\n",
    "    \n",
    "checkpointer = keras.callbacks.ModelCheckpoint(\n",
    "  \"unet.h5\", \n",
    "  verbose = 1, \n",
    "  save_best_only = True\n",
    ")\n",
    "\n",
    "\n",
    "#strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "if (os.path.exists(\"unet.h5\")):\n",
    "    model = keras.models.load_model(\"unet.h5\",\n",
    "    custom_objects = {\n",
    "      \"jaccard_distance_loss\": jaccard_distance_loss,\n",
    "      \"dice_coef\": dice_coef\n",
    "    }\n",
    "  )\n",
    "  \n",
    "else:\n",
    "\n",
    "    model = unet_model()\n",
    "    adam_opt = keras.optimizers.Adam(learning_rate = 0.001)\n",
    "    model.compile(optimizer = adam_opt, loss = jaccard_distance_loss, metrics = [dice_coef])\n",
    "  \n",
    "\n",
    "    fit = model.fit((X_train, Y_train), \n",
    "    steps_per_epoch = steps_per_epoch, \n",
    "    epochs = 100,\n",
    "    validation_data = (X_val, Y_val),\n",
    "    callbacks = [\n",
    "      checkpointer,\n",
    "      reduce_learning_rate\n",
    "    ]\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow] *",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
