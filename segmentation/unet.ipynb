{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import os\n",
    "import shutil\n",
    "import re\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage\n",
    "from skimage.io import imread, imsave\n",
    "from skimage.transform import resize\n",
    "from skimage import img_as_float32, img_as_ubyte\n",
    "import pickle\n",
    "import cv2\n",
    "\n",
    "from albumentations import (\n",
    "    Compose, HorizontalFlip, ShiftScaleRotate, ElasticTransform,\n",
    "    RandomBrightness, RandomContrast, RandomGamma\n",
    ")\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "  except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AugmentationSequence(keras.utils.Sequence):\n",
    "  def __init__(self, x_set, y_set, batch_size, augmentations):\n",
    "    self.x, self.y = x_set, y_set\n",
    "    self.batch_size = batch_size\n",
    "    self.augment = augmentations\n",
    "\n",
    "  def __len__(self):\n",
    "    return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "    batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "    \n",
    "    aug_x = np.zeros(batch_x.shape)\n",
    "    aug_y = np.zeros(batch_y.shape)\n",
    "    \n",
    "    for idx in range(batch_x.shape[0]):\n",
    "      aug = self.augment(image = batch_x[idx,:,:,:], mask = batch_y[idx,:,:,:])\n",
    "      aug_x[idx,:,:,:] = aug[\"image\"]\n",
    "      aug_y[idx,:,:,:] = aug[\"mask\"]\n",
    "    \n",
    "    return aug_x, aug_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n"
     ]
    }
   ],
   "source": [
    "img_size = 400\n",
    "file_pi = open('input/train_dataset.pkl', 'rb') \n",
    "train_generator =  pickle.load(file_pi)\n",
    "\n",
    "X_val = np.load('input/X_val.npy')\n",
    "Y_val = np.load('input/Y_val.npy')\n",
    "\n",
    "with open('input/epochs.txt', 'r') as file:\n",
    "    steps_per_epoch = file.read().rstrip()\n",
    "\n",
    "steps_per_epoch = int(steps_per_epoch)\n",
    "\n",
    "print(steps_per_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOSS Functions\n",
    "def jaccard_distance_loss(y_true, y_pred, smooth = 100):\n",
    "    intersection = keras.backend.sum(keras.backend.abs(y_true * y_pred), axis = -1)\n",
    "    union = keras.backend.sum(keras.backend.abs(y_true) + keras.backend.abs(y_pred), axis = -1)\n",
    "    jac = (intersection + smooth) / (union - intersection + smooth)\n",
    "    loss = (1 - jac) * smooth\n",
    "    return loss\n",
    "\n",
    "def dice_coef(y_true, y_pred, smooth = 1):\n",
    "    intersection = keras.backend.sum(keras.backend.abs(y_true * y_pred), axis = -1)\n",
    "    union = keras.backend.sum(keras.backend.abs(y_true), -1) + keras.backend.sum(keras.backend.abs(y_pred), -1)\n",
    "    return (2. * intersection + smooth) / (union + smooth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet_model():\n",
    "  \n",
    "  input_img = keras.layers.Input((img_size, img_size, 1), name = \"img\")\n",
    "  \n",
    "  # Contract #1\n",
    "  c1 = keras.layers.Conv2D(16, (3, 3), kernel_initializer = \"he_uniform\", padding = \"same\")(input_img)\n",
    "  c1 = keras.layers.BatchNormalization()(c1)\n",
    "  c1 = keras.layers.Activation(\"relu\")(c1)\n",
    "  c1 = keras.layers.Dropout(0.1)(c1)\n",
    "  c1 = keras.layers.Conv2D(16, (3, 3), kernel_initializer = \"he_uniform\", padding = \"same\")(c1)\n",
    "  c1 = keras.layers.BatchNormalization()(c1)\n",
    "  c1 = keras.layers.Activation(\"relu\")(c1)\n",
    "  p1 = keras.layers.MaxPooling2D((2, 2))(c1)\n",
    "  \n",
    "  # Contract #2\n",
    "  c2 = keras.layers.Conv2D(32, (3, 3), kernel_initializer = \"he_uniform\", padding = \"same\")(p1)\n",
    "  c2 = keras.layers.BatchNormalization()(c2)\n",
    "  c2 = keras.layers.Activation(\"relu\")(c2)\n",
    "  c2 = keras.layers.Dropout(0.2)(c2)\n",
    "  c2 = keras.layers.Conv2D(32, (3, 3), kernel_initializer = \"he_uniform\", padding = \"same\")(c2)\n",
    "  c2 = keras.layers.BatchNormalization()(c2)\n",
    "  c2 = keras.layers.Activation(\"relu\")(c2)\n",
    "  p2 = keras.layers.MaxPooling2D((2, 2))(c2)\n",
    "  \n",
    "  # Contract #3\n",
    "  c3 = keras.layers.Conv2D(64, (3, 3), kernel_initializer = \"he_uniform\", padding = \"same\")(p2)\n",
    "  c3 = keras.layers.BatchNormalization()(c3)\n",
    "  c3 = keras.layers.Activation(\"relu\")(c3)\n",
    "  c3 = keras.layers.Dropout(0.3)(c3)\n",
    "  c3 = keras.layers.Conv2D(64, (3, 3), kernel_initializer = \"he_uniform\", padding = \"same\")(c3)\n",
    "  c3 = keras.layers.BatchNormalization()(c3)\n",
    "  c3 = keras.layers.Activation(\"relu\")(c3)\n",
    "  p3 = keras.layers.MaxPooling2D((2, 2))(c3)\n",
    "  \n",
    "  # Contract #4\n",
    "  c4 = keras.layers.Conv2D(128, (3, 3), kernel_initializer = \"he_uniform\", padding = \"same\")(p3)\n",
    "  c4 = keras.layers.BatchNormalization()(c4)\n",
    "  c4 = keras.layers.Activation(\"relu\")(c4)\n",
    "  c4 = keras.layers.Dropout(0.4)(c4)\n",
    "  c4 = keras.layers.Conv2D(128, (3, 3), kernel_initializer = \"he_uniform\", padding = \"same\")(c4)\n",
    "  c4 = keras.layers.BatchNormalization()(c4)\n",
    "  c4 = keras.layers.Activation(\"relu\")(c4)\n",
    "  p4 = keras.layers.MaxPooling2D((2, 2))(c4)\n",
    "  \n",
    "  # Middle\n",
    "  c5 = keras.layers.Conv2D(256, (3, 3), kernel_initializer = \"he_uniform\", padding = \"same\")(p4)\n",
    "  c5 = keras.layers.BatchNormalization()(c5)\n",
    "  c5 = keras.layers.Activation(\"relu\")(c5)\n",
    "  c5 = keras.layers.Dropout(0.5)(c5)\n",
    "  c5 = keras.layers.Conv2D(256, (3, 3), kernel_initializer = \"he_uniform\", padding = \"same\")(c5)\n",
    "  c5 = keras.layers.BatchNormalization()(c5)\n",
    "  c5 = keras.layers.Activation(\"relu\")(c5)\n",
    "  \n",
    "  # Expand (upscale) #1\n",
    "  u6 = keras.layers.Conv2DTranspose(128, (3, 3), strides = (2, 2), padding = \"same\")(c5)\n",
    "  u6 = keras.layers.concatenate([u6, c4])\n",
    "  c6 = keras.layers.Conv2D(128, (3, 3), kernel_initializer = \"he_uniform\", padding = \"same\")(u6)\n",
    "  c6 = keras.layers.BatchNormalization()(c6)\n",
    "  c6 = keras.layers.Activation(\"relu\")(c6)\n",
    "  c6 = keras.layers.Dropout(0.5)(c6)\n",
    "  c6 = keras.layers.Conv2D(128, (3, 3), kernel_initializer = \"he_uniform\", padding = \"same\")(c6)\n",
    "  c6 = keras.layers.BatchNormalization()(c6)\n",
    "  c6 = keras.layers.Activation(\"relu\")(c6)\n",
    "  \n",
    "  # Expand (upscale) #2\n",
    "  u7 = keras.layers.Conv2DTranspose(64, (3, 3), strides = (2, 2), padding = \"same\")(c6)\n",
    "  u7 = keras.layers.concatenate([u7, c3])\n",
    "  c7 = keras.layers.Conv2D(64, (3, 3), kernel_initializer = \"he_uniform\", padding = \"same\")(u7)\n",
    "  c7 = keras.layers.BatchNormalization()(c7)\n",
    "  c7 = keras.layers.Activation(\"relu\")(c7)\n",
    "  c7 = keras.layers.Dropout(0.5)(c7)\n",
    "  c7 = keras.layers.Conv2D(64, (3, 3), kernel_initializer = \"he_uniform\", padding = \"same\")(c7)\n",
    "  c7 = keras.layers.BatchNormalization()(c7)\n",
    "  c7 = keras.layers.Activation(\"relu\")(c7)\n",
    "  \n",
    "  # Expand (upscale) #3\n",
    "  u8 = keras.layers.Conv2DTranspose(32, (3, 3), strides = (2, 2), padding = \"same\")(c7)\n",
    "  u8 = keras.layers.concatenate([u8, c2])\n",
    "  c8 = keras.layers.Conv2D(32, (3, 3), kernel_initializer = \"he_uniform\", padding = \"same\")(u8)\n",
    "  c8 = keras.layers.BatchNormalization()(c8)\n",
    "  c8 = keras.layers.Activation(\"relu\")(c8)\n",
    "  c8 = keras.layers.Dropout(0.5)(c8)\n",
    "  c8 = keras.layers.Conv2D(32, (3, 3), kernel_initializer = \"he_uniform\", padding = \"same\")(c8)\n",
    "  c8 = keras.layers.BatchNormalization()(c8)\n",
    "  c8 = keras.layers.Activation(\"relu\")(c8)\n",
    "  \n",
    "  # Expand (upscale) #4\n",
    "  u9 = keras.layers.Conv2DTranspose(16, (3, 3), strides = (2, 2), padding = \"same\")(c8)\n",
    "  u9 = keras.layers.concatenate([u9, c1])\n",
    "  c9 = keras.layers.Conv2D(16, (3, 3), kernel_initializer = \"he_uniform\", padding = \"same\")(u9)\n",
    "  c9 = keras.layers.BatchNormalization()(c9)\n",
    "  c9 = keras.layers.Activation(\"relu\")(c9)\n",
    "  c9 = keras.layers.Dropout(0.5)(c9)\n",
    "  c9 = keras.layers.Conv2D(16, (3, 3), kernel_initializer = \"he_uniform\", padding = \"same\")(c9)\n",
    "  c9 = keras.layers.BatchNormalization()(c9)\n",
    "  c9 = keras.layers.Activation(\"relu\")(c9)\n",
    "  \n",
    "  output = keras.layers.Conv2D(1, (1, 1), activation = \"sigmoid\")(c9)\n",
    "  model = keras.Model(inputs = [input_img], outputs = [output])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning rate + Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "Epoch 1/100\n",
      "90/90 [==============================] - 232s 2s/step - loss: 0.3728 - dice_coef: 0.7772 - val_loss: 0.4153 - val_dice_coef: 0.7673\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.41531, saving model to unet.h5\n",
      "Epoch 2/100\n",
      "90/90 [==============================] - 210s 2s/step - loss: 0.2154 - dice_coef: 0.8713 - val_loss: 0.3709 - val_dice_coef: 0.8037\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.41531 to 0.37091, saving model to unet.h5\n",
      "Epoch 3/100\n",
      "90/90 [==============================] - 211s 2s/step - loss: 0.1620 - dice_coef: 0.9079 - val_loss: 0.2758 - val_dice_coef: 0.8535\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.37091 to 0.27584, saving model to unet.h5\n",
      "Epoch 4/100\n",
      "90/90 [==============================] - 209s 2s/step - loss: 0.1423 - dice_coef: 0.9220 - val_loss: 0.2940 - val_dice_coef: 0.8465\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.27584\n",
      "Epoch 5/100\n",
      "90/90 [==============================] - 213s 2s/step - loss: 0.1300 - dice_coef: 0.9305 - val_loss: 0.2783 - val_dice_coef: 0.8566\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.27584\n",
      "Epoch 6/100\n",
      "90/90 [==============================] - 212s 2s/step - loss: 0.1227 - dice_coef: 0.9353 - val_loss: 0.2558 - val_dice_coef: 0.8683\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.27584 to 0.25581, saving model to unet.h5\n",
      "Epoch 7/100\n",
      "90/90 [==============================] - 212s 2s/step - loss: 0.1199 - dice_coef: 0.9375 - val_loss: 0.2545 - val_dice_coef: 0.8694\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.25581 to 0.25447, saving model to unet.h5\n",
      "Epoch 8/100\n",
      "90/90 [==============================] - 219s 2s/step - loss: 0.1150 - dice_coef: 0.9403 - val_loss: 0.2634 - val_dice_coef: 0.8658\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.25447\n",
      "Epoch 9/100\n",
      "90/90 [==============================] - 211s 2s/step - loss: 0.1140 - dice_coef: 0.9411 - val_loss: 0.2533 - val_dice_coef: 0.8709\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.25447 to 0.25326, saving model to unet.h5\n",
      "Epoch 10/100\n",
      "90/90 [==============================] - 225s 2s/step - loss: 0.1079 - dice_coef: 0.9445 - val_loss: 0.2390 - val_dice_coef: 0.8783\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.25326 to 0.23897, saving model to unet.h5\n",
      "Epoch 11/100\n",
      "90/90 [==============================] - 212s 2s/step - loss: 0.1048 - dice_coef: 0.9460 - val_loss: 0.2686 - val_dice_coef: 0.8634\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.23897\n",
      "Epoch 12/100\n",
      "90/90 [==============================] - 210s 2s/step - loss: 0.1002 - dice_coef: 0.9485 - val_loss: 0.2525 - val_dice_coef: 0.8717\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.23897\n",
      "Epoch 13/100\n",
      "90/90 [==============================] - 214s 2s/step - loss: 0.1012 - dice_coef: 0.9481 - val_loss: 0.2536 - val_dice_coef: 0.8711\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.23897\n",
      "Epoch 14/100\n",
      "90/90 [==============================] - 236s 3s/step - loss: 0.0972 - dice_coef: 0.9502 - val_loss: 0.2418 - val_dice_coef: 0.8774\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.23897\n",
      "Epoch 15/100\n",
      "90/90 [==============================] - 215s 2s/step - loss: 0.0960 - dice_coef: 0.9509 - val_loss: 0.2356 - val_dice_coef: 0.8802\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.23897 to 0.23561, saving model to unet.h5\n",
      "Epoch 16/100\n",
      "90/90 [==============================] - 213s 2s/step - loss: 0.1003 - dice_coef: 0.9488 - val_loss: 0.2542 - val_dice_coef: 0.8709\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.23561\n",
      "Epoch 17/100\n",
      "90/90 [==============================] - 224s 2s/step - loss: 0.0952 - dice_coef: 0.9514 - val_loss: 0.2214 - val_dice_coef: 0.8875\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.23561 to 0.22143, saving model to unet.h5\n",
      "Epoch 18/100\n",
      "90/90 [==============================] - 213s 2s/step - loss: 0.0911 - dice_coef: 0.9535 - val_loss: 0.2253 - val_dice_coef: 0.8857\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.22143\n",
      "Epoch 19/100\n",
      "90/90 [==============================] - 200s 2s/step - loss: 0.0879 - dice_coef: 0.9551 - val_loss: 0.2302 - val_dice_coef: 0.8834\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.22143\n",
      "Epoch 20/100\n",
      "90/90 [==============================] - 209s 2s/step - loss: 0.0882 - dice_coef: 0.9550 - val_loss: 0.2359 - val_dice_coef: 0.8803\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.22143\n",
      "Epoch 21/100\n",
      "90/90 [==============================] - 221s 2s/step - loss: 0.0874 - dice_coef: 0.9554 - val_loss: 0.2212 - val_dice_coef: 0.8880\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.22143 to 0.22116, saving model to unet.h5\n",
      "Epoch 22/100\n",
      "90/90 [==============================] - 213s 2s/step - loss: 0.0859 - dice_coef: 0.9562 - val_loss: 0.2284 - val_dice_coef: 0.8843\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.22116\n",
      "Epoch 23/100\n",
      "90/90 [==============================] - 221s 2s/step - loss: 0.0830 - dice_coef: 0.9577 - val_loss: 0.2534 - val_dice_coef: 0.8716\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.22116\n",
      "Epoch 24/100\n",
      "90/90 [==============================] - 229s 3s/step - loss: 0.0847 - dice_coef: 0.9569 - val_loss: 0.2302 - val_dice_coef: 0.8833\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.22116\n",
      "Epoch 25/100\n",
      "90/90 [==============================] - 215s 2s/step - loss: 0.0851 - dice_coef: 0.9567 - val_loss: 0.2281 - val_dice_coef: 0.8844\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.22116\n",
      "Epoch 26/100\n",
      "90/90 [==============================] - 210s 2s/step - loss: 0.0833 - dice_coef: 0.9576 - val_loss: 0.2150 - val_dice_coef: 0.8911\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.22116 to 0.21495, saving model to unet.h5\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 27/100\n",
      "90/90 [==============================] - 222s 2s/step - loss: 0.0784 - dice_coef: 0.9601 - val_loss: 0.2214 - val_dice_coef: 0.8878\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.21495\n",
      "Epoch 28/100\n",
      "90/90 [==============================] - 223s 2s/step - loss: 0.0791 - dice_coef: 0.9597 - val_loss: 0.2108 - val_dice_coef: 0.8933\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.21495 to 0.21079, saving model to unet.h5\n",
      "Epoch 29/100\n",
      "90/90 [==============================] - 212s 2s/step - loss: 0.0785 - dice_coef: 0.9601 - val_loss: 0.2135 - val_dice_coef: 0.8918\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.21079\n",
      "Epoch 30/100\n",
      "90/90 [==============================] - 207s 2s/step - loss: 0.0777 - dice_coef: 0.9605 - val_loss: 0.2288 - val_dice_coef: 0.8841\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.21079\n",
      "Epoch 31/100\n",
      "90/90 [==============================] - 213s 2s/step - loss: 0.0780 - dice_coef: 0.9603 - val_loss: 0.2132 - val_dice_coef: 0.8919\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.21079\n",
      "Epoch 32/100\n",
      "90/90 [==============================] - 221s 2s/step - loss: 0.0769 - dice_coef: 0.9609 - val_loss: 0.2232 - val_dice_coef: 0.8870\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.21079\n",
      "Epoch 33/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 209s 2s/step - loss: 0.0753 - dice_coef: 0.9617 - val_loss: 0.2145 - val_dice_coef: 0.8914\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.21079\n",
      "Epoch 34/100\n",
      "90/90 [==============================] - 206s 2s/step - loss: 0.0771 - dice_coef: 0.9608 - val_loss: 0.2255 - val_dice_coef: 0.8857\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.21079\n",
      "Epoch 35/100\n",
      "90/90 [==============================] - 210s 2s/step - loss: 0.0758 - dice_coef: 0.9615 - val_loss: 0.2366 - val_dice_coef: 0.8803\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.21079\n",
      "Epoch 36/100\n",
      "90/90 [==============================] - 212s 2s/step - loss: 0.0767 - dice_coef: 0.9610 - val_loss: 0.2293 - val_dice_coef: 0.8840\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.21079\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 37/100\n",
      "90/90 [==============================] - 213s 2s/step - loss: 0.0742 - dice_coef: 0.9623 - val_loss: 0.2211 - val_dice_coef: 0.8880\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.21079\n",
      "Epoch 38/100\n",
      "90/90 [==============================] - 216s 2s/step - loss: 0.0731 - dice_coef: 0.9628 - val_loss: 0.2219 - val_dice_coef: 0.8877\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.21079\n",
      "Epoch 39/100\n",
      "90/90 [==============================] - 211s 2s/step - loss: 0.0738 - dice_coef: 0.9625 - val_loss: 0.2319 - val_dice_coef: 0.8828\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.21079\n",
      "Epoch 40/100\n",
      "90/90 [==============================] - 213s 2s/step - loss: 0.0738 - dice_coef: 0.9625 - val_loss: 0.2274 - val_dice_coef: 0.8849\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.21079\n",
      "Epoch 41/100\n",
      "90/90 [==============================] - 209s 2s/step - loss: 0.0707 - dice_coef: 0.9640 - val_loss: 0.2213 - val_dice_coef: 0.8880\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.21079\n",
      "Epoch 42/100\n",
      "90/90 [==============================] - 219s 2s/step - loss: 0.0713 - dice_coef: 0.9637 - val_loss: 0.2319 - val_dice_coef: 0.8826\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.21079\n",
      "Epoch 43/100\n",
      "90/90 [==============================] - 212s 2s/step - loss: 0.0735 - dice_coef: 0.9626 - val_loss: 0.2452 - val_dice_coef: 0.8759\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.21079\n",
      "Epoch 44/100\n",
      "90/90 [==============================] - 217s 2s/step - loss: 0.0722 - dice_coef: 0.9633 - val_loss: 0.2315 - val_dice_coef: 0.8828\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.21079\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 45/100\n",
      "90/90 [==============================] - 210s 2s/step - loss: 0.0714 - dice_coef: 0.9637 - val_loss: 0.2320 - val_dice_coef: 0.8824\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.21079\n",
      "Epoch 46/100\n",
      "90/90 [==============================] - 210s 2s/step - loss: 0.0717 - dice_coef: 0.9635 - val_loss: 0.2396 - val_dice_coef: 0.8787\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.21079\n",
      "Epoch 47/100\n",
      "90/90 [==============================] - 200s 2s/step - loss: 0.0714 - dice_coef: 0.9637 - val_loss: 0.2445 - val_dice_coef: 0.8762\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.21079\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 48/100\n",
      "90/90 [==============================] - 207s 2s/step - loss: 0.0699 - dice_coef: 0.9645 - val_loss: 0.2410 - val_dice_coef: 0.8780\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.21079\n",
      "Epoch 49/100\n",
      "90/90 [==============================] - 205s 2s/step - loss: 0.0707 - dice_coef: 0.9641 - val_loss: 0.2251 - val_dice_coef: 0.8861\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.21079\n",
      "Epoch 50/100\n",
      "90/90 [==============================] - 215s 2s/step - loss: 0.0688 - dice_coef: 0.9650 - val_loss: 0.2406 - val_dice_coef: 0.8782\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.21079\n",
      "Epoch 51/100\n",
      "90/90 [==============================] - 205s 2s/step - loss: 0.0690 - dice_coef: 0.9649 - val_loss: 0.2242 - val_dice_coef: 0.8864\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.21079\n",
      "Epoch 52/100\n",
      "90/90 [==============================] - 215s 2s/step - loss: 0.0693 - dice_coef: 0.9648 - val_loss: 0.2334 - val_dice_coef: 0.8819\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.21079\n",
      "Epoch 53/100\n",
      "90/90 [==============================] - 205s 2s/step - loss: 0.0701 - dice_coef: 0.9644 - val_loss: 0.2233 - val_dice_coef: 0.8870\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.21079\n",
      "\n",
      "Epoch 00053: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 54/100\n",
      "90/90 [==============================] - 215s 2s/step - loss: 0.0690 - dice_coef: 0.9649 - val_loss: 0.2443 - val_dice_coef: 0.8763\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.21079\n",
      "Epoch 55/100\n",
      "90/90 [==============================] - 208s 2s/step - loss: 0.0707 - dice_coef: 0.9640 - val_loss: 0.2443 - val_dice_coef: 0.8763\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.21079\n",
      "Epoch 56/100\n",
      "90/90 [==============================] - 216s 2s/step - loss: 0.0702 - dice_coef: 0.9643 - val_loss: 0.2450 - val_dice_coef: 0.8760\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.21079\n",
      "\n",
      "Epoch 00056: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 57/100\n",
      "90/90 [==============================] - 217s 2s/step - loss: 0.0679 - dice_coef: 0.9654 - val_loss: 0.2433 - val_dice_coef: 0.8770\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.21079\n",
      "Epoch 58/100\n",
      "90/90 [==============================] - 208s 2s/step - loss: 0.0700 - dice_coef: 0.9644 - val_loss: 0.2449 - val_dice_coef: 0.8760\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.21079\n",
      "Epoch 59/100\n",
      "90/90 [==============================] - 214s 2s/step - loss: 0.0697 - dice_coef: 0.9645 - val_loss: 0.2452 - val_dice_coef: 0.8758\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.21079\n",
      "Epoch 60/100\n",
      "90/90 [==============================] - 207s 2s/step - loss: 0.0687 - dice_coef: 0.9651 - val_loss: 0.2441 - val_dice_coef: 0.8764\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.21079\n",
      "\n",
      "Epoch 00060: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "Epoch 61/100\n",
      "90/90 [==============================] - 206s 2s/step - loss: 0.0685 - dice_coef: 0.9652 - val_loss: 0.2447 - val_dice_coef: 0.8761\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.21079\n",
      "Epoch 62/100\n",
      "90/90 [==============================] - 216s 2s/step - loss: 0.0700 - dice_coef: 0.9644 - val_loss: 0.2446 - val_dice_coef: 0.8761\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.21079\n",
      "Epoch 63/100\n",
      "90/90 [==============================] - 223s 2s/step - loss: 0.0681 - dice_coef: 0.9654 - val_loss: 0.2447 - val_dice_coef: 0.8761\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.21079\n",
      "\n",
      "Epoch 00063: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "Epoch 64/100\n",
      "90/90 [==============================] - 218s 2s/step - loss: 0.0671 - dice_coef: 0.9659 - val_loss: 0.2439 - val_dice_coef: 0.8765\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.21079\n",
      "Epoch 65/100\n",
      "90/90 [==============================] - 224s 2s/step - loss: 0.0684 - dice_coef: 0.9652 - val_loss: 0.2445 - val_dice_coef: 0.8762\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.21079\n",
      "Epoch 66/100\n",
      "90/90 [==============================] - 207s 2s/step - loss: 0.0686 - dice_coef: 0.9651 - val_loss: 0.2448 - val_dice_coef: 0.8761\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.21079\n",
      "Epoch 67/100\n",
      "90/90 [==============================] - 208s 2s/step - loss: 0.0688 - dice_coef: 0.9650 - val_loss: 0.2451 - val_dice_coef: 0.8759\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.21079\n",
      "\n",
      "Epoch 00067: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "Epoch 68/100\n",
      "90/90 [==============================] - 206s 2s/step - loss: 0.0695 - dice_coef: 0.9647 - val_loss: 0.2452 - val_dice_coef: 0.8758\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.21079\n",
      "Epoch 69/100\n",
      "90/90 [==============================] - 211s 2s/step - loss: 0.0693 - dice_coef: 0.9648 - val_loss: 0.2454 - val_dice_coef: 0.8757\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.21079\n",
      "Epoch 70/100\n",
      "90/90 [==============================] - 204s 2s/step - loss: 0.0690 - dice_coef: 0.9649 - val_loss: 0.2456 - val_dice_coef: 0.8757\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.21079\n",
      "\n",
      "Epoch 00070: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
      "Epoch 71/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 211s 2s/step - loss: 0.0688 - dice_coef: 0.9650 - val_loss: 0.2439 - val_dice_coef: 0.8765\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.21079\n",
      "Epoch 72/100\n",
      "90/90 [==============================] - 216s 2s/step - loss: 0.0693 - dice_coef: 0.9647 - val_loss: 0.2448 - val_dice_coef: 0.8761\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.21079\n",
      "Epoch 73/100\n",
      "90/90 [==============================] - 209s 2s/step - loss: 0.0688 - dice_coef: 0.9650 - val_loss: 0.2453 - val_dice_coef: 0.8758\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.21079\n",
      "\n",
      "Epoch 00073: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
      "Epoch 74/100\n",
      "90/90 [==============================] - 221s 2s/step - loss: 0.0704 - dice_coef: 0.9642 - val_loss: 0.2455 - val_dice_coef: 0.8757\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.21079\n",
      "Epoch 75/100\n",
      "90/90 [==============================] - 209s 2s/step - loss: 0.0685 - dice_coef: 0.9652 - val_loss: 0.2456 - val_dice_coef: 0.8757\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.21079\n",
      "Epoch 76/100\n",
      "90/90 [==============================] - 206s 2s/step - loss: 0.0695 - dice_coef: 0.9646 - val_loss: 0.2461 - val_dice_coef: 0.8754\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.21079\n",
      "\n",
      "Epoch 00076: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
      "Epoch 77/100\n",
      "90/90 [==============================] - 208s 2s/step - loss: 0.0693 - dice_coef: 0.9647 - val_loss: 0.2454 - val_dice_coef: 0.8757\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.21079\n",
      "Epoch 78/100\n",
      "90/90 [==============================] - 208s 2s/step - loss: 0.0703 - dice_coef: 0.9643 - val_loss: 0.2458 - val_dice_coef: 0.8755\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.21079\n",
      "Epoch 79/100\n",
      "90/90 [==============================] - 211s 2s/step - loss: 0.0685 - dice_coef: 0.9652 - val_loss: 0.2458 - val_dice_coef: 0.8755\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.21079\n",
      "\n",
      "Epoch 00079: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
      "Epoch 80/100\n",
      "90/90 [==============================] - 214s 2s/step - loss: 0.0688 - dice_coef: 0.9650 - val_loss: 0.2454 - val_dice_coef: 0.8757\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.21079\n",
      "Epoch 81/100\n",
      "90/90 [==============================] - 209s 2s/step - loss: 0.0684 - dice_coef: 0.9652 - val_loss: 0.2457 - val_dice_coef: 0.8756\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.21079\n",
      "Epoch 82/100\n",
      "90/90 [==============================] - 213s 2s/step - loss: 0.0687 - dice_coef: 0.9651 - val_loss: 0.2452 - val_dice_coef: 0.8758\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.21079\n",
      "\n",
      "Epoch 00082: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
      "Epoch 83/100\n",
      "90/90 [==============================] - 213s 2s/step - loss: 0.0686 - dice_coef: 0.9651 - val_loss: 0.2444 - val_dice_coef: 0.8762\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.21079\n",
      "Epoch 84/100\n",
      "90/90 [==============================] - 209s 2s/step - loss: 0.0710 - dice_coef: 0.9639 - val_loss: 0.2450 - val_dice_coef: 0.8759\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.21079\n",
      "Epoch 85/100\n",
      "90/90 [==============================] - 213s 2s/step - loss: 0.0689 - dice_coef: 0.9650 - val_loss: 0.2447 - val_dice_coef: 0.8761\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.21079\n",
      "\n",
      "Epoch 00085: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
      "Epoch 86/100\n",
      "90/90 [==============================] - 207s 2s/step - loss: 0.0703 - dice_coef: 0.9643 - val_loss: 0.2458 - val_dice_coef: 0.8755\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.21079\n",
      "Epoch 87/100\n",
      "90/90 [==============================] - 208s 2s/step - loss: 0.0697 - dice_coef: 0.9646 - val_loss: 0.2458 - val_dice_coef: 0.8755\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.21079\n",
      "Epoch 88/100\n",
      "90/90 [==============================] - 211s 2s/step - loss: 0.0679 - dice_coef: 0.9655 - val_loss: 0.2456 - val_dice_coef: 0.8756\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.21079\n",
      "\n",
      "Epoch 00088: ReduceLROnPlateau reducing learning rate to 1.525878978725359e-08.\n",
      "Epoch 89/100\n",
      "90/90 [==============================] - 210s 2s/step - loss: 0.0694 - dice_coef: 0.9647 - val_loss: 0.2452 - val_dice_coef: 0.8758\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.21079\n",
      "Epoch 90/100\n",
      "90/90 [==============================] - 210s 2s/step - loss: 0.0685 - dice_coef: 0.9652 - val_loss: 0.2452 - val_dice_coef: 0.8758\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.21079\n",
      "Epoch 91/100\n",
      "90/90 [==============================] - 216s 2s/step - loss: 0.0694 - dice_coef: 0.9647 - val_loss: 0.2441 - val_dice_coef: 0.8764\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.21079\n",
      "\n",
      "Epoch 00091: ReduceLROnPlateau reducing learning rate to 7.629394893626795e-09.\n",
      "Epoch 92/100\n",
      "90/90 [==============================] - 207s 2s/step - loss: 0.0695 - dice_coef: 0.9647 - val_loss: 0.2451 - val_dice_coef: 0.8759\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.21079\n",
      "Epoch 93/100\n",
      "90/90 [==============================] - 213s 2s/step - loss: 0.0690 - dice_coef: 0.9649 - val_loss: 0.2448 - val_dice_coef: 0.8761\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.21079\n",
      "Epoch 94/100\n",
      "90/90 [==============================] - 215s 2s/step - loss: 0.0695 - dice_coef: 0.9647 - val_loss: 0.2448 - val_dice_coef: 0.8760\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.21079\n",
      "\n",
      "Epoch 00094: ReduceLROnPlateau reducing learning rate to 3.814697446813398e-09.\n",
      "Epoch 95/100\n",
      "90/90 [==============================] - 220s 2s/step - loss: 0.0689 - dice_coef: 0.9650 - val_loss: 0.2450 - val_dice_coef: 0.8760\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.21079\n",
      "Epoch 96/100\n",
      "90/90 [==============================] - 218s 2s/step - loss: 0.0682 - dice_coef: 0.9653 - val_loss: 0.2450 - val_dice_coef: 0.8759\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.21079\n",
      "Epoch 97/100\n",
      "90/90 [==============================] - 214s 2s/step - loss: 0.0684 - dice_coef: 0.9652 - val_loss: 0.2449 - val_dice_coef: 0.8760\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.21079\n",
      "\n",
      "Epoch 00097: ReduceLROnPlateau reducing learning rate to 1.907348723406699e-09.\n",
      "Epoch 98/100\n",
      "90/90 [==============================] - 206s 2s/step - loss: 0.0680 - dice_coef: 0.9654 - val_loss: 0.2443 - val_dice_coef: 0.8763\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.21079\n",
      "Epoch 99/100\n",
      "90/90 [==============================] - 219s 2s/step - loss: 0.0693 - dice_coef: 0.9648 - val_loss: 0.2444 - val_dice_coef: 0.8762\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.21079\n",
      "Epoch 100/100\n",
      "90/90 [==============================] - 205s 2s/step - loss: 0.0683 - dice_coef: 0.9653 - val_loss: 0.2451 - val_dice_coef: 0.8759\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.21079\n",
      "\n",
      "Epoch 00100: ReduceLROnPlateau reducing learning rate to 9.536743617033494e-10.\n"
     ]
    }
   ],
   "source": [
    "reduce_learning_rate = keras.callbacks.ReduceLROnPlateau(\n",
    "  monitor = \"loss\", \n",
    "  factor = 0.5, \n",
    "  patience = 3, \n",
    "  verbose = 1\n",
    ")\n",
    "\n",
    "checkpointer = keras.callbacks.ModelCheckpoint(\n",
    "  \"unet.h5\", \n",
    "  verbose = 1, \n",
    "  save_best_only = True\n",
    ")\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "if (os.path.exists(\"unet.h5\")):\n",
    "  model = keras.models.load_model(\"unet.h5\",\n",
    "    custom_objects = {\n",
    "      \"jaccard_distance_loss\": jaccard_distance_loss,\n",
    "      \"dice_coef\": dice_coef\n",
    "    }\n",
    "  )\n",
    "  \n",
    "else:\n",
    "  with strategy.scope():\n",
    "    model = unet_model()\n",
    "    adam_opt = keras.optimizers.Adam(learning_rate = 0.001)\n",
    "    model.compile(optimizer = adam_opt, loss = jaccard_distance_loss, metrics = [dice_coef])\n",
    "    \n",
    "  fit = model.fit(train_generator, \n",
    "    steps_per_epoch = steps_per_epoch, \n",
    "    epochs = 100,\n",
    "    validation_data = (X_val, Y_val),\n",
    "    callbacks = [\n",
    "      checkpointer,\n",
    "      reduce_learning_rate\n",
    "    ]\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "img (InputLayer)                [(None, 400, 400, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 400, 400, 16) 160         img[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 400, 400, 16) 64          conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 400, 400, 16) 0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 400, 400, 16) 0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 400, 400, 16) 2320        dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 400, 400, 16) 64          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 400, 400, 16) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 200, 200, 16) 0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 200, 200, 32) 4640        max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 200, 200, 32) 128         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 200, 200, 32) 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 200, 200, 32) 0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 200, 200, 32) 9248        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 200, 200, 32) 128         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 200, 200, 32) 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 100, 100, 32) 0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 100, 100, 64) 18496       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 100, 100, 64) 256         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 100, 100, 64) 0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 100, 100, 64) 0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 100, 100, 64) 36928       dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 100, 100, 64) 256         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 100, 100, 64) 0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 50, 50, 64)   0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 50, 50, 128)  73856       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 50, 50, 128)  512         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 50, 50, 128)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 50, 50, 128)  0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 50, 50, 128)  147584      dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 50, 50, 128)  512         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 50, 50, 128)  0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 25, 25, 128)  0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 25, 25, 256)  295168      max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 25, 25, 256)  1024        conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 25, 25, 256)  0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 25, 25, 256)  0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 25, 25, 256)  590080      dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 25, 25, 256)  1024        conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 25, 25, 256)  0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose (Conv2DTranspo (None, 50, 50, 128)  295040      activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 50, 50, 256)  0           conv2d_transpose[0][0]           \n",
      "                                                                 activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 50, 50, 128)  295040      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 50, 50, 128)  512         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 50, 50, 128)  0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 50, 50, 128)  0           activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 50, 50, 128)  147584      dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 50, 50, 128)  512         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 50, 50, 128)  0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTrans (None, 100, 100, 64) 73792       activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 100, 100, 128 0           conv2d_transpose_1[0][0]         \n",
      "                                                                 activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 100, 100, 64) 73792       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 100, 100, 64) 256         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 100, 100, 64) 0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 100, 100, 64) 0           activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 100, 100, 64) 36928       dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 100, 100, 64) 256         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 100, 100, 64) 0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTrans (None, 200, 200, 32) 18464       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 200, 200, 64) 0           conv2d_transpose_2[0][0]         \n",
      "                                                                 activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 200, 200, 32) 18464       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 200, 200, 32) 128         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 200, 200, 32) 0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 200, 200, 32) 0           activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 200, 200, 32) 9248        dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 200, 200, 32) 128         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 200, 200, 32) 0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTrans (None, 400, 400, 16) 4624        activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 400, 400, 32) 0           conv2d_transpose_3[0][0]         \n",
      "                                                                 activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 400, 400, 16) 4624        concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 400, 400, 16) 64          conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 400, 400, 16) 0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 400, 400, 16) 0           activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 400, 400, 16) 2320        dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 400, 400, 16) 64          conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 400, 400, 16) 0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 400, 400, 1)  17          activation_17[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 2,164,305\n",
      "Trainable params: 2,161,361\n",
      "Non-trainable params: 2,944\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load('input/X_train.npy')\n",
    "Y_train = np.load('input/Y_train.npy')\n",
    "\n",
    "iou_train, dice_train = model.evaluate(X_train, Y_train, verbose = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iou_val, dice_val = model.evaluate(X_val, Y_val, verbose = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.load('input/X_test.npy')\n",
    "Y_test = np.load('input/Y_test.npy')\n",
    "\n",
    "iou_test, dice_test = model.evaluate(X_test, Y_test, verbose = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Jaccard distance (IoU) train: %f\" % iou_train)\n",
    "print(\"Dice coeffient train: %f\" % dice_train)\n",
    "print(\"Jaccard distance (IoU) validation: %f\" % iou_val)\n",
    "print(\"Dice coeffient validation: %f\" % dice_val)\n",
    "print(\"Jaccard distance (IoU) test: %f\" % iou_test)\n",
    "print(\"Dice coeffient test: %f\" % dice_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nimages = X_train.shape[0]\n",
    "iou_train = []\n",
    "dice_train = []\n",
    "for idx in range(nimages):\n",
    "  iou, dice = model.evaluate(X_train[idx:idx+1,:,:], Y_train[idx:idx+1,:,:], verbose = False)\n",
    "  iou_train.append(iou)\n",
    "  dice_train.append(dice)\n",
    "\n",
    "print(\"Jaccard distance (IoU) train: %f (+-%f)\" % (np.mean(iou_train), np.std(iou_train)))\n",
    "print(\"Dice coeffient train: %f (+-%f)\" % (np.mean(dice_train), np.std(dice_train)))\n",
    "\n",
    "nimages = X_val.shape[0]\n",
    "iou_val = []\n",
    "dice_val = []\n",
    "for idx in range(nimages):\n",
    "  iou, dice = model.evaluate(X_val[idx:idx+1,:,:], Y_val[idx:idx+1,:,:], verbose = False)\n",
    "  iou_val.append(iou)\n",
    "  dice_val.append(dice)\n",
    "\n",
    "print(\"Jaccard distance (IoU) validation: %f (+-%f)\" % (np.mean(iou_val), np.std(iou_val)))\n",
    "print(\"Dice coeffient validation: %f (+-%f)\" % (np.mean(dice_val), np.std(dice_val)))\n",
    "\n",
    "\n",
    "nimages = X_test.shape[0]\n",
    "iou_test = []\n",
    "dice_test = []\n",
    "for idx in range(nimages):\n",
    "  iou, dice = model.evaluate(X_test[idx:idx+1,:,:], Y_test[idx:idx+1,:,:], verbose = False)\n",
    "  iou_test.append(iou)\n",
    "  dice_test.append(dice)\n",
    "\n",
    "print(\"Jaccard distance (IoU) test: %f (+-%f)\" % (np.mean(iou_test), np.std(iou_test)))\n",
    "print(\"Dice coeffient test: %f (+-%f)\" % (np.mean(dice_test), np.std(dice_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nimages = X_test.shape[0]\n",
    "iou_test = []\n",
    "dice_test = []\n",
    "with tf.device(\"/gpu:1\"):\n",
    "  for idx in range(nimages):\n",
    "    iou, dice = model.evaluate(X_test[idx:idx+1,:,:], Y_test[idx:idx+1,:,:], verbose = False)\n",
    "    iou_test.append(iou)\n",
    "    dice_test.append(dice)\n",
    "\n",
    "print(\"Jaccard distance (IoU) test: %f (+-%f)\" % (np.mean(iou_test), np.std(iou_test)))\n",
    "print(\"Dice coeffient test: %f (+-%f)\" % (np.mean(dice_test), np.std(dice_test)))\n",
    "\n",
    "\n",
    "iou_shenzhen = []\n",
    "dice_shenzhen = []\n",
    "with tf.device(\"/gpu:1\"):\n",
    "  for idx in shenzhen_test_ids:\n",
    "    iou, dice = model.evaluate(X_test[idx:idx+1,:,:], Y_test[idx:idx+1,:,:], verbose = False)\n",
    "    iou_shenzhen.append(iou)\n",
    "    dice_shenzhen.append(dice)\n",
    "\n",
    "print(\"Jaccard distance (IoU) Shenzhen: %f (+-%f)\" % (np.mean(iou_shenzhen), np.std(iou_shenzhen)))\n",
    "print(\"Dice coeffient Shenzhen: %f (+-%f)\" % (np.mean(dice_shenzhen), np.std(dice_shenzhen)))\n",
    "\n",
    "\n",
    "iou_montgomery = []\n",
    "dice_montgomery = []\n",
    "with tf.device(\"/gpu:1\"):\n",
    "  for idx in montgomery_test_ids:\n",
    "    iou, dice = model.evaluate(X_test[idx:idx+1,:,:], Y_test[idx:idx+1,:,:], verbose = False)\n",
    "    iou_montgomery.append(iou)\n",
    "    dice_montgomery.append(dice)\n",
    "\n",
    "print(\"Jaccard distance (IoU) Montgomery: %f (+-%f)\" % (np.mean(iou_montgomery), np.std(iou_montgomery)))\n",
    "print(\"Dice coeffient Montgomery: %f (+-%f)\" % (np.mean(dice_montgomery), np.std(dice_montgomery)))\n",
    "\n",
    "\n",
    "iou_jsrt = []\n",
    "dice_jsrt = []\n",
    "with tf.device(\"/gpu:1\"):\n",
    "  for idx in jsrt_test_ids:\n",
    "    iou, dice = model.evaluate(X_test[idx:idx+1,:,:], Y_test[idx:idx+1,:,:], verbose = False)\n",
    "    iou_jsrt.append(iou)\n",
    "    dice_jsrt.append(dice)\n",
    "\n",
    "print(\"Jaccard distance (IoU) JSRT: %f (+-%f)\" % (np.mean(iou_jsrt), np.std(iou_jsrt)))\n",
    "print(\"Dice coeffient JSRT: %f (+-%f)\" % (np.mean(dice_jsrt), np.std(dice_jsrt)))\n",
    "\n",
    "\n",
    "iou_v7labs = []\n",
    "dice_v7labs = []\n",
    "with tf.device(\"/gpu:1\"):\n",
    "  for idx in v7labs_test_ids:\n",
    "    iou, dice = model.evaluate(X_test[idx:idx+1,:,:], Y_test[idx:idx+1,:,:], verbose = False)\n",
    "    iou_v7labs.append(iou)\n",
    "    dice_v7labs.append(dice)\n",
    "\n",
    "print(\"Jaccard distance (IoU) v7labs: %f (+-%f)\" % (np.mean(iou_v7labs), np.std(iou_v7labs)))\n",
    "print(\"Dice coeffient v7labs: %f (+-%f)\" % (np.mean(dice_v7labs), np.std(dice_v7labs)))\n",
    "\n",
    "\n",
    "\n",
    "iou_manual = []\n",
    "dice_manual = []\n",
    "with tf.device(\"/gpu:1\"):\n",
    "  for idx in other_test_ids:\n",
    "    iou, dice = model.evaluate(X_test[idx:idx+1,:,:], Y_test[idx:idx+1,:,:], verbose = False)\n",
    "    iou_manual.append(iou)\n",
    "    dice_manual.append(dice)\n",
    "\n",
    "print(\"Jaccard distance (IoU) manual: %f (+-%f)\" % (np.mean(iou_manual), np.std(iou_manual)))\n",
    "print(\"Dice coeffient manual: %f (+-%f)\" % (np.mean(dice_manual), np.std(dice_manual)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 50\n",
    "test_img = X_test[idx,:,:,:].reshape((1, img_size, img_size, 1))\n",
    "test_mask = Y_test[idx,:,:,:].reshape((1, img_size, img_size, 1))\n",
    "pred_mask = model.predict(test_img)\n",
    "pred_mask = np.uint8(pred_mask > 0.5)\n",
    "post_pred_mask = skimage.morphology.erosion(pred_mask[0,:,:,0], skimage.morphology.square(5))\n",
    "post_pred_mask = skimage.morphology.dilation(post_pred_mask, skimage.morphology.square(10))\n",
    "\n",
    "f = plt.figure(figsize = (20, 10))\n",
    "f.add_subplot(1, 4, 1)\n",
    "plt.imshow(img_as_ubyte(test_img[0,:,:,0]), cmap = \"gray\")\n",
    "f.add_subplot(1, 4, 2)\n",
    "plt.imshow(test_mask[0,:,:,0], cmap = \"gray\")\n",
    "f.add_subplot(1, 4, 3)\n",
    "plt.imshow(pred_mask[0,:,:,0], cmap = \"gray\")\n",
    "f.add_subplot(1, 4, 4)\n",
    "plt.imshow(post_pred_mask, cmap = \"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image(img, mask):\n",
    "  crop_mask = mask > 0\n",
    "  m, n = mask.shape\n",
    "  crop_mask0, crop_mask1 = crop_mask.any(0), crop_mask.any(1)\n",
    "  col_start, col_end = crop_mask0.argmax(), n - crop_mask0[::-1].argmax()\n",
    "  row_start, row_end = crop_mask1.argmax(), m - crop_mask1[::-1].argmax()\n",
    "  return img[row_start:row_end, col_start:col_end], mask[row_start:row_end, col_start:col_end]\n",
    "  \n",
    "#idx = 70\n",
    "idx = 68\n",
    "test_img = X_test[idx,:,:,:].reshape((1, img_size, img_size, 1))\n",
    "test_mask = Y_test[idx,:,:,:].reshape((1, img_size, img_size, 1))\n",
    "pred_mask = model.predict(test_img)[0,:,:,0]\n",
    "pred_mask = np.uint8(pred_mask > 0.5)\n",
    "open_pred_mask = skimage.morphology.erosion(pred_mask, skimage.morphology.square(5))\n",
    "open_pred_mask = skimage.morphology.dilation(open_pred_mask, skimage.morphology.square(5))\n",
    "post_pred_mask = skimage.morphology.dilation(open_pred_mask, skimage.morphology.square(5))\n",
    "\n",
    "crop_img, crop_mask = crop_image(test_img[0,:,:,0], post_pred_mask)\n",
    "\n",
    "crop_img_masked = crop_img * crop_mask\n",
    "\n",
    "f = plt.figure()\n",
    "f.add_subplot(2, 2, 1)\n",
    "plt.imshow(img_as_ubyte(test_img[0,:,:,0]), cmap = \"gray\")\n",
    "f.add_subplot(2, 2, 2)\n",
    "plt.imshow(post_pred_mask, cmap = \"gray\")\n",
    "f.add_subplot(2, 2, 3)\n",
    "plt.imshow(img_as_ubyte(crop_img), cmap = \"gray\")\n",
    "f.add_subplot(2, 2, 4)\n",
    "plt.imshow(crop_mask, cmap = \"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize = (20, 20))\n",
    "f.add_subplot(1, 1, 1)\n",
    "plt.imshow(img_as_ubyte(test_img[0,:,:,0]), cmap = \"gray\")\n",
    "plt.setp(plt.gcf().get_axes(), xticks=[], yticks=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize = (20, 20))\n",
    "f.add_subplot(1, 1, 1)\n",
    "plt.imshow(pred_mask, cmap = \"gray\")\n",
    "plt.setp(plt.gcf().get_axes(), xticks=[], yticks=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize = (20, 20))\n",
    "f.add_subplot(1, 1, 1)\n",
    "plt.imshow(open_pred_mask, cmap = \"gray\")\n",
    "plt.setp(plt.gcf().get_axes(), xticks=[], yticks=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize = (20, 20))\n",
    "f.add_subplot(1, 1, 1)\n",
    "plt.imshow(post_pred_mask, cmap = \"gray\")\n",
    "plt.setp(plt.gcf().get_axes(), xticks=[], yticks=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize = (20, 20))\n",
    "f.add_subplot(1, 1, 1)\n",
    "plt.imshow(crop_img_masked, cmap = \"gray\")\n",
    "plt.setp(plt.gcf().get_axes(), xticks=[], yticks=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize = (20, 20))\n",
    "f.add_subplot(1, 1, 1)\n",
    "plt.imshow(img_as_ubyte(test_img[5,:,:,0]), cmap = \"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_folders = [\"A1\", \"A2\", \"Cohen\", \"Eurorad\", \"Radiopedia\", \"RSNA\", \"Actualmed\", \"Figure1\", \"Kaggle_CRD\"]\n",
    "pathogen_folders = [\"Opacity\", \"COVID-19\", \"Normal\"]\n",
    "dest_folder = \"2_Images_Seg\"\n",
    "\n",
    "if os.path.isdir(dest_folder):\n",
    "  shutil.rmtree(dest_folder)\n",
    "\n",
    "if not os.path.isdir(dest_folder):\n",
    "  os.makedirs(dest_folder)\n",
    "  \n",
    "if not os.path.isdir(os.path.join(dest_folder, \"masks\")):\n",
    "  os.makedirs(os.path.join(dest_folder, \"masks\"))\n",
    "  \n",
    "for target in [\"train\", \"test\"]:\n",
    "  for pathogen in pathogen_folders:\n",
    "    pathogen_folder = os.path.join(dest_folder, target, pathogen)\n",
    "    if not os.path.isdir(pathogen_folder):\n",
    "      os.makedirs(pathogen_folder)\n",
    "\n",
    "with tf.device(\"/gpu:0\"):\n",
    "  targets = [\"train\", \"test\"]\n",
    "  for target in targets:\n",
    "    pathogens = os.listdir(os.path.join(root_folder, target))\n",
    "    for pathogen in pathogens:\n",
    "      images = os.listdir(os.path.join(root_folder, target, pathogen))\n",
    "      for image in images:\n",
    "        source, _, pid, offset, _ = re.split(\"[_.]\", image)\n",
    "        img = imread(os.path.join(root_folder, target, pathogen, image))\n",
    "        img = img_as_float32(img).reshape((1, img_size, img_size, 1))\n",
    "        mask_pred = model.predict(img)[0,:,:,0]\n",
    "        mask_pred = np.uint8(mask_pred > 0.5)\n",
    "        mask_pred = skimage.morphology.erosion(mask_pred, skimage.morphology.square(5))\n",
    "        mask_pred = skimage.morphology.dilation(mask_pred, skimage.morphology.square(10))\n",
    "        img = img_as_ubyte(img[0,:,:,0])\n",
    "\n",
    "        crop_img, crop_mask = crop_image(img, mask_pred)\n",
    "        \n",
    "        mask_filename = \"%s_%s_%s_%s_%s.png\" % (target, source, pathogen, pid, offset)\n",
    "        img_filename = \"%s_%s_%s_%s.png\" % (source, pathogen, pid, offset)\n",
    "\n",
    "        wdt, hgt = crop_img.shape\n",
    "        if wdt < 200 or hgt < 200:\n",
    "          print(img_filename)\n",
    "          #continue\n",
    "\n",
    "        crop_img = crop_img * crop_mask\n",
    "\n",
    "        crop_img = cv2.resize(crop_img, (300, 300), interpolation = cv2.INTER_CUBIC)\n",
    "        crop_mask = cv2.resize(crop_mask, (300, 300), interpolation = cv2.INTER_CUBIC)\n",
    "\n",
    "        imsave(os.path.join(dest_folder, \"masks\", mask_filename), crop_mask * 255)\n",
    "        imsave(os.path.join(dest_folder, target, pathogen, img_filename), crop_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow] *",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
