{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import os\n",
    "import shutil\n",
    "import re\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage\n",
    "from skimage.io import imread, imsave\n",
    "from skimage.transform import resize\n",
    "from skimage import img_as_float32, img_as_ubyte\n",
    "import pickle\n",
    "import cv2\n",
    "\n",
    "from albumentations import (\n",
    "    Compose, HorizontalFlip, ShiftScaleRotate, ElasticTransform,\n",
    "    RandomBrightness, RandomContrast, RandomGamma\n",
    ")\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "  except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AugmentationSequence(keras.utils.Sequence):\n",
    "  def __init__(self, x_set, y_set, batch_size, augmentations):\n",
    "    self.x, self.y = x_set, y_set\n",
    "    self.batch_size = batch_size\n",
    "    self.augment = augmentations\n",
    "\n",
    "  def __len__(self):\n",
    "    return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "    batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "    \n",
    "    aug_x = np.zeros(batch_x.shape)\n",
    "    aug_y = np.zeros(batch_y.shape)\n",
    "    \n",
    "    for idx in range(batch_x.shape[0]):\n",
    "      aug = self.augment(image = batch_x[idx,:,:,:], mask = batch_y[idx,:,:,:])\n",
    "      aug_x[idx,:,:,:] = aug[\"image\"]\n",
    "      aug_y[idx,:,:,:] = aug[\"mask\"]\n",
    "    \n",
    "    return aug_x, aug_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n"
     ]
    }
   ],
   "source": [
    "img_size = 400\n",
    "file_pi = open('input/train_dataset.pkl', 'rb') \n",
    "train_generator =  pickle.load(file_pi)\n",
    "\n",
    "X_val = np.load('input/X_val.npy')\n",
    "Y_val = np.load('input/Y_val.npy')\n",
    "\n",
    "with open('input/epochs.txt', 'r') as file:\n",
    "    steps_per_epoch = file.read().rstrip()\n",
    "\n",
    "steps_per_epoch = int(steps_per_epoch)\n",
    "\n",
    "print(steps_per_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOSS Functions\n",
    "def jaccard_distance_loss(y_true, y_pred, smooth = 100):\n",
    "    intersection = keras.backend.sum(keras.backend.abs(y_true * y_pred), axis = -1)\n",
    "    union = keras.backend.sum(keras.backend.abs(y_true) + keras.backend.abs(y_pred), axis = -1)\n",
    "    jac = (intersection + smooth) / (union - intersection + smooth)\n",
    "    loss = (1 - jac) * smooth\n",
    "    return loss\n",
    "\n",
    "def dice_coef(y_true, y_pred, smooth = 1):\n",
    "    intersection = keras.backend.sum(keras.backend.abs(y_true * y_pred), axis = -1)\n",
    "    union = keras.backend.sum(keras.backend.abs(y_true), -1) + keras.backend.sum(keras.backend.abs(y_pred), -1)\n",
    "    return (2. * intersection + smooth) / (union + smooth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet_model():\n",
    "  \n",
    "  input_img = keras.layers.Input((img_size, img_size, 1), name = \"img\")\n",
    "  \n",
    "  # Contract #1\n",
    "  c1 = keras.layers.Conv2D(16, (3, 3), kernel_initializer = \"he_uniform\", padding = \"same\")(input_img)\n",
    "  c1 = keras.layers.BatchNormalization()(c1)\n",
    "  c1 = keras.layers.Activation(\"relu\")(c1)\n",
    "  c1 = keras.layers.Dropout(0.1)(c1)\n",
    "  c1 = keras.layers.Conv2D(16, (3, 3), kernel_initializer = \"he_uniform\", padding = \"same\")(c1)\n",
    "  c1 = keras.layers.BatchNormalization()(c1)\n",
    "  c1 = keras.layers.Activation(\"relu\")(c1)\n",
    "  p1 = keras.layers.MaxPooling2D((2, 2))(c1)\n",
    "  \n",
    "  # Contract #2\n",
    "  c2 = keras.layers.Conv2D(32, (3, 3), kernel_initializer = \"he_uniform\", padding = \"same\")(p1)\n",
    "  c2 = keras.layers.BatchNormalization()(c2)\n",
    "  c2 = keras.layers.Activation(\"relu\")(c2)\n",
    "  c2 = keras.layers.Dropout(0.2)(c2)\n",
    "  c2 = keras.layers.Conv2D(32, (3, 3), kernel_initializer = \"he_uniform\", padding = \"same\")(c2)\n",
    "  c2 = keras.layers.BatchNormalization()(c2)\n",
    "  c2 = keras.layers.Activation(\"relu\")(c2)\n",
    "  p2 = keras.layers.MaxPooling2D((2, 2))(c2)\n",
    "  \n",
    "  # Contract #3\n",
    "  c3 = keras.layers.Conv2D(64, (3, 3), kernel_initializer = \"he_uniform\", padding = \"same\")(p2)\n",
    "  c3 = keras.layers.BatchNormalization()(c3)\n",
    "  c3 = keras.layers.Activation(\"relu\")(c3)\n",
    "  c3 = keras.layers.Dropout(0.3)(c3)\n",
    "  c3 = keras.layers.Conv2D(64, (3, 3), kernel_initializer = \"he_uniform\", padding = \"same\")(c3)\n",
    "  c3 = keras.layers.BatchNormalization()(c3)\n",
    "  c3 = keras.layers.Activation(\"relu\")(c3)\n",
    "  p3 = keras.layers.MaxPooling2D((2, 2))(c3)\n",
    "  \n",
    "  # Contract #4\n",
    "  c4 = keras.layers.Conv2D(128, (3, 3), kernel_initializer = \"he_uniform\", padding = \"same\")(p3)\n",
    "  c4 = keras.layers.BatchNormalization()(c4)\n",
    "  c4 = keras.layers.Activation(\"relu\")(c4)\n",
    "  c4 = keras.layers.Dropout(0.4)(c4)\n",
    "  c4 = keras.layers.Conv2D(128, (3, 3), kernel_initializer = \"he_uniform\", padding = \"same\")(c4)\n",
    "  c4 = keras.layers.BatchNormalization()(c4)\n",
    "  c4 = keras.layers.Activation(\"relu\")(c4)\n",
    "  p4 = keras.layers.MaxPooling2D((2, 2))(c4)\n",
    "  \n",
    "  # Middle\n",
    "  c5 = keras.layers.Conv2D(256, (3, 3), kernel_initializer = \"he_uniform\", padding = \"same\")(p4)\n",
    "  c5 = keras.layers.BatchNormalization()(c5)\n",
    "  c5 = keras.layers.Activation(\"relu\")(c5)\n",
    "  c5 = keras.layers.Dropout(0.5)(c5)\n",
    "  c5 = keras.layers.Conv2D(256, (3, 3), kernel_initializer = \"he_uniform\", padding = \"same\")(c5)\n",
    "  c5 = keras.layers.BatchNormalization()(c5)\n",
    "  c5 = keras.layers.Activation(\"relu\")(c5)\n",
    "  \n",
    "  # Expand (upscale) #1\n",
    "  u6 = keras.layers.Conv2DTranspose(128, (3, 3), strides = (2, 2), padding = \"same\")(c5)\n",
    "  u6 = keras.layers.concatenate([u6, c4])\n",
    "  c6 = keras.layers.Conv2D(128, (3, 3), kernel_initializer = \"he_uniform\", padding = \"same\")(u6)\n",
    "  c6 = keras.layers.BatchNormalization()(c6)\n",
    "  c6 = keras.layers.Activation(\"relu\")(c6)\n",
    "  c6 = keras.layers.Dropout(0.5)(c6)\n",
    "  c6 = keras.layers.Conv2D(128, (3, 3), kernel_initializer = \"he_uniform\", padding = \"same\")(c6)\n",
    "  c6 = keras.layers.BatchNormalization()(c6)\n",
    "  c6 = keras.layers.Activation(\"relu\")(c6)\n",
    "  \n",
    "  # Expand (upscale) #2\n",
    "  u7 = keras.layers.Conv2DTranspose(64, (3, 3), strides = (2, 2), padding = \"same\")(c6)\n",
    "  u7 = keras.layers.concatenate([u7, c3])\n",
    "  c7 = keras.layers.Conv2D(64, (3, 3), kernel_initializer = \"he_uniform\", padding = \"same\")(u7)\n",
    "  c7 = keras.layers.BatchNormalization()(c7)\n",
    "  c7 = keras.layers.Activation(\"relu\")(c7)\n",
    "  c7 = keras.layers.Dropout(0.5)(c7)\n",
    "  c7 = keras.layers.Conv2D(64, (3, 3), kernel_initializer = \"he_uniform\", padding = \"same\")(c7)\n",
    "  c7 = keras.layers.BatchNormalization()(c7)\n",
    "  c7 = keras.layers.Activation(\"relu\")(c7)\n",
    "  \n",
    "  # Expand (upscale) #3\n",
    "  u8 = keras.layers.Conv2DTranspose(32, (3, 3), strides = (2, 2), padding = \"same\")(c7)\n",
    "  u8 = keras.layers.concatenate([u8, c2])\n",
    "  c8 = keras.layers.Conv2D(32, (3, 3), kernel_initializer = \"he_uniform\", padding = \"same\")(u8)\n",
    "  c8 = keras.layers.BatchNormalization()(c8)\n",
    "  c8 = keras.layers.Activation(\"relu\")(c8)\n",
    "  c8 = keras.layers.Dropout(0.5)(c8)\n",
    "  c8 = keras.layers.Conv2D(32, (3, 3), kernel_initializer = \"he_uniform\", padding = \"same\")(c8)\n",
    "  c8 = keras.layers.BatchNormalization()(c8)\n",
    "  c8 = keras.layers.Activation(\"relu\")(c8)\n",
    "  \n",
    "  # Expand (upscale) #4\n",
    "  u9 = keras.layers.Conv2DTranspose(16, (3, 3), strides = (2, 2), padding = \"same\")(c8)\n",
    "  u9 = keras.layers.concatenate([u9, c1])\n",
    "  c9 = keras.layers.Conv2D(16, (3, 3), kernel_initializer = \"he_uniform\", padding = \"same\")(u9)\n",
    "  c9 = keras.layers.BatchNormalization()(c9)\n",
    "  c9 = keras.layers.Activation(\"relu\")(c9)\n",
    "  c9 = keras.layers.Dropout(0.5)(c9)\n",
    "  c9 = keras.layers.Conv2D(16, (3, 3), kernel_initializer = \"he_uniform\", padding = \"same\")(c9)\n",
    "  c9 = keras.layers.BatchNormalization()(c9)\n",
    "  c9 = keras.layers.Activation(\"relu\")(c9)\n",
    "  \n",
    "  output = keras.layers.Conv2D(1, (1, 1), activation = \"sigmoid\")(c9)\n",
    "  model = keras.Model(inputs = [input_img], outputs = [output])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning rate + Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "Epoch 1/100\n",
      "90/90 [==============================] - 232s 2s/step - loss: 0.3728 - dice_coef: 0.7772 - val_loss: 0.4153 - val_dice_coef: 0.7673\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.41531, saving model to unet.h5\n",
      "Epoch 2/100\n",
      "90/90 [==============================] - 210s 2s/step - loss: 0.2154 - dice_coef: 0.8713 - val_loss: 0.3709 - val_dice_coef: 0.8037\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.41531 to 0.37091, saving model to unet.h5\n",
      "Epoch 3/100\n",
      "90/90 [==============================] - 211s 2s/step - loss: 0.1620 - dice_coef: 0.9079 - val_loss: 0.2758 - val_dice_coef: 0.8535\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.37091 to 0.27584, saving model to unet.h5\n",
      "Epoch 4/100\n",
      "90/90 [==============================] - 209s 2s/step - loss: 0.1423 - dice_coef: 0.9220 - val_loss: 0.2940 - val_dice_coef: 0.8465\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.27584\n",
      "Epoch 5/100\n",
      "90/90 [==============================] - 213s 2s/step - loss: 0.1300 - dice_coef: 0.9305 - val_loss: 0.2783 - val_dice_coef: 0.8566\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.27584\n",
      "Epoch 6/100\n",
      "90/90 [==============================] - 212s 2s/step - loss: 0.1227 - dice_coef: 0.9353 - val_loss: 0.2558 - val_dice_coef: 0.8683\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.27584 to 0.25581, saving model to unet.h5\n",
      "Epoch 7/100\n",
      "90/90 [==============================] - 212s 2s/step - loss: 0.1199 - dice_coef: 0.9375 - val_loss: 0.2545 - val_dice_coef: 0.8694\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.25581 to 0.25447, saving model to unet.h5\n",
      "Epoch 8/100\n",
      "90/90 [==============================] - 219s 2s/step - loss: 0.1150 - dice_coef: 0.9403 - val_loss: 0.2634 - val_dice_coef: 0.8658\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.25447\n",
      "Epoch 9/100\n",
      "90/90 [==============================] - 211s 2s/step - loss: 0.1140 - dice_coef: 0.9411 - val_loss: 0.2533 - val_dice_coef: 0.8709\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.25447 to 0.25326, saving model to unet.h5\n",
      "Epoch 10/100\n",
      "90/90 [==============================] - 225s 2s/step - loss: 0.1079 - dice_coef: 0.9445 - val_loss: 0.2390 - val_dice_coef: 0.8783\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.25326 to 0.23897, saving model to unet.h5\n",
      "Epoch 11/100\n",
      "90/90 [==============================] - 212s 2s/step - loss: 0.1048 - dice_coef: 0.9460 - val_loss: 0.2686 - val_dice_coef: 0.8634\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.23897\n",
      "Epoch 12/100\n",
      "90/90 [==============================] - 210s 2s/step - loss: 0.1002 - dice_coef: 0.9485 - val_loss: 0.2525 - val_dice_coef: 0.8717\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.23897\n",
      "Epoch 13/100\n",
      "90/90 [==============================] - 214s 2s/step - loss: 0.1012 - dice_coef: 0.9481 - val_loss: 0.2536 - val_dice_coef: 0.8711\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.23897\n",
      "Epoch 14/100\n",
      "90/90 [==============================] - 236s 3s/step - loss: 0.0972 - dice_coef: 0.9502 - val_loss: 0.2418 - val_dice_coef: 0.8774\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.23897\n",
      "Epoch 15/100\n",
      "90/90 [==============================] - 215s 2s/step - loss: 0.0960 - dice_coef: 0.9509 - val_loss: 0.2356 - val_dice_coef: 0.8802\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.23897 to 0.23561, saving model to unet.h5\n",
      "Epoch 16/100\n",
      "90/90 [==============================] - 213s 2s/step - loss: 0.1003 - dice_coef: 0.9488 - val_loss: 0.2542 - val_dice_coef: 0.8709\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.23561\n",
      "Epoch 17/100\n",
      "90/90 [==============================] - 224s 2s/step - loss: 0.0952 - dice_coef: 0.9514 - val_loss: 0.2214 - val_dice_coef: 0.8875\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.23561 to 0.22143, saving model to unet.h5\n",
      "Epoch 18/100\n",
      "90/90 [==============================] - 213s 2s/step - loss: 0.0911 - dice_coef: 0.9535 - val_loss: 0.2253 - val_dice_coef: 0.8857\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.22143\n",
      "Epoch 19/100\n",
      "90/90 [==============================] - 200s 2s/step - loss: 0.0879 - dice_coef: 0.9551 - val_loss: 0.2302 - val_dice_coef: 0.8834\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.22143\n",
      "Epoch 20/100\n",
      "90/90 [==============================] - 209s 2s/step - loss: 0.0882 - dice_coef: 0.9550 - val_loss: 0.2359 - val_dice_coef: 0.8803\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.22143\n",
      "Epoch 21/100\n",
      "90/90 [==============================] - 221s 2s/step - loss: 0.0874 - dice_coef: 0.9554 - val_loss: 0.2212 - val_dice_coef: 0.8880\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.22143 to 0.22116, saving model to unet.h5\n",
      "Epoch 22/100\n",
      "90/90 [==============================] - 213s 2s/step - loss: 0.0859 - dice_coef: 0.9562 - val_loss: 0.2284 - val_dice_coef: 0.8843\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.22116\n",
      "Epoch 23/100\n",
      "90/90 [==============================] - 221s 2s/step - loss: 0.0830 - dice_coef: 0.9577 - val_loss: 0.2534 - val_dice_coef: 0.8716\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.22116\n",
      "Epoch 24/100\n",
      "90/90 [==============================] - 229s 3s/step - loss: 0.0847 - dice_coef: 0.9569 - val_loss: 0.2302 - val_dice_coef: 0.8833\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.22116\n",
      "Epoch 25/100\n",
      "90/90 [==============================] - 215s 2s/step - loss: 0.0851 - dice_coef: 0.9567 - val_loss: 0.2281 - val_dice_coef: 0.8844\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.22116\n",
      "Epoch 26/100\n",
      "90/90 [==============================] - 210s 2s/step - loss: 0.0833 - dice_coef: 0.9576 - val_loss: 0.2150 - val_dice_coef: 0.8911\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.22116 to 0.21495, saving model to unet.h5\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 27/100\n",
      "90/90 [==============================] - 222s 2s/step - loss: 0.0784 - dice_coef: 0.9601 - val_loss: 0.2214 - val_dice_coef: 0.8878\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.21495\n",
      "Epoch 28/100\n",
      "90/90 [==============================] - 223s 2s/step - loss: 0.0791 - dice_coef: 0.9597 - val_loss: 0.2108 - val_dice_coef: 0.8933\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.21495 to 0.21079, saving model to unet.h5\n",
      "Epoch 29/100\n",
      "90/90 [==============================] - 212s 2s/step - loss: 0.0785 - dice_coef: 0.9601 - val_loss: 0.2135 - val_dice_coef: 0.8918\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.21079\n",
      "Epoch 30/100\n",
      "90/90 [==============================] - 207s 2s/step - loss: 0.0777 - dice_coef: 0.9605 - val_loss: 0.2288 - val_dice_coef: 0.8841\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.21079\n",
      "Epoch 31/100\n",
      "90/90 [==============================] - 213s 2s/step - loss: 0.0780 - dice_coef: 0.9603 - val_loss: 0.2132 - val_dice_coef: 0.8919\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.21079\n",
      "Epoch 32/100\n",
      "90/90 [==============================] - 221s 2s/step - loss: 0.0769 - dice_coef: 0.9609 - val_loss: 0.2232 - val_dice_coef: 0.8870\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.21079\n",
      "Epoch 33/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 209s 2s/step - loss: 0.0753 - dice_coef: 0.9617 - val_loss: 0.2145 - val_dice_coef: 0.8914\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.21079\n",
      "Epoch 34/100\n",
      "90/90 [==============================] - 206s 2s/step - loss: 0.0771 - dice_coef: 0.9608 - val_loss: 0.2255 - val_dice_coef: 0.8857\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.21079\n",
      "Epoch 35/100\n",
      "90/90 [==============================] - 210s 2s/step - loss: 0.0758 - dice_coef: 0.9615 - val_loss: 0.2366 - val_dice_coef: 0.8803\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.21079\n",
      "Epoch 36/100\n",
      "90/90 [==============================] - 212s 2s/step - loss: 0.0767 - dice_coef: 0.9610 - val_loss: 0.2293 - val_dice_coef: 0.8840\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.21079\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 37/100\n",
      "90/90 [==============================] - 213s 2s/step - loss: 0.0742 - dice_coef: 0.9623 - val_loss: 0.2211 - val_dice_coef: 0.8880\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.21079\n",
      "Epoch 38/100\n",
      "90/90 [==============================] - 216s 2s/step - loss: 0.0731 - dice_coef: 0.9628 - val_loss: 0.2219 - val_dice_coef: 0.8877\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.21079\n",
      "Epoch 39/100\n",
      "90/90 [==============================] - 211s 2s/step - loss: 0.0738 - dice_coef: 0.9625 - val_loss: 0.2319 - val_dice_coef: 0.8828\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.21079\n",
      "Epoch 40/100\n",
      "90/90 [==============================] - 213s 2s/step - loss: 0.0738 - dice_coef: 0.9625 - val_loss: 0.2274 - val_dice_coef: 0.8849\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.21079\n",
      "Epoch 41/100\n",
      "90/90 [==============================] - 209s 2s/step - loss: 0.0707 - dice_coef: 0.9640 - val_loss: 0.2213 - val_dice_coef: 0.8880\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.21079\n",
      "Epoch 42/100\n",
      "90/90 [==============================] - 219s 2s/step - loss: 0.0713 - dice_coef: 0.9637 - val_loss: 0.2319 - val_dice_coef: 0.8826\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.21079\n",
      "Epoch 43/100\n",
      "90/90 [==============================] - 212s 2s/step - loss: 0.0735 - dice_coef: 0.9626 - val_loss: 0.2452 - val_dice_coef: 0.8759\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.21079\n",
      "Epoch 44/100\n",
      "90/90 [==============================] - 217s 2s/step - loss: 0.0722 - dice_coef: 0.9633 - val_loss: 0.2315 - val_dice_coef: 0.8828\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.21079\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 45/100\n",
      "90/90 [==============================] - 210s 2s/step - loss: 0.0714 - dice_coef: 0.9637 - val_loss: 0.2320 - val_dice_coef: 0.8824\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.21079\n",
      "Epoch 46/100\n",
      "90/90 [==============================] - 210s 2s/step - loss: 0.0717 - dice_coef: 0.9635 - val_loss: 0.2396 - val_dice_coef: 0.8787\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.21079\n",
      "Epoch 47/100\n",
      "90/90 [==============================] - 200s 2s/step - loss: 0.0714 - dice_coef: 0.9637 - val_loss: 0.2445 - val_dice_coef: 0.8762\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.21079\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 48/100\n",
      "90/90 [==============================] - 207s 2s/step - loss: 0.0699 - dice_coef: 0.9645 - val_loss: 0.2410 - val_dice_coef: 0.8780\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.21079\n",
      "Epoch 49/100\n",
      "90/90 [==============================] - 205s 2s/step - loss: 0.0707 - dice_coef: 0.9641 - val_loss: 0.2251 - val_dice_coef: 0.8861\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.21079\n",
      "Epoch 50/100\n",
      "90/90 [==============================] - 215s 2s/step - loss: 0.0688 - dice_coef: 0.9650 - val_loss: 0.2406 - val_dice_coef: 0.8782\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.21079\n",
      "Epoch 51/100\n",
      "90/90 [==============================] - 205s 2s/step - loss: 0.0690 - dice_coef: 0.9649 - val_loss: 0.2242 - val_dice_coef: 0.8864\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.21079\n",
      "Epoch 52/100\n",
      "90/90 [==============================] - 215s 2s/step - loss: 0.0693 - dice_coef: 0.9648 - val_loss: 0.2334 - val_dice_coef: 0.8819\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.21079\n",
      "Epoch 53/100\n",
      "90/90 [==============================] - 205s 2s/step - loss: 0.0701 - dice_coef: 0.9644 - val_loss: 0.2233 - val_dice_coef: 0.8870\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.21079\n",
      "\n",
      "Epoch 00053: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 54/100\n",
      "90/90 [==============================] - 215s 2s/step - loss: 0.0690 - dice_coef: 0.9649 - val_loss: 0.2443 - val_dice_coef: 0.8763\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.21079\n",
      "Epoch 55/100\n",
      "90/90 [==============================] - 208s 2s/step - loss: 0.0707 - dice_coef: 0.9640 - val_loss: 0.2443 - val_dice_coef: 0.8763\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.21079\n",
      "Epoch 56/100\n",
      "90/90 [==============================] - 216s 2s/step - loss: 0.0702 - dice_coef: 0.9643 - val_loss: 0.2450 - val_dice_coef: 0.8760\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.21079\n",
      "\n",
      "Epoch 00056: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 57/100\n",
      "90/90 [==============================] - 217s 2s/step - loss: 0.0679 - dice_coef: 0.9654 - val_loss: 0.2433 - val_dice_coef: 0.8770\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.21079\n",
      "Epoch 58/100\n",
      "90/90 [==============================] - 208s 2s/step - loss: 0.0700 - dice_coef: 0.9644 - val_loss: 0.2449 - val_dice_coef: 0.8760\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.21079\n",
      "Epoch 59/100\n",
      "90/90 [==============================] - 214s 2s/step - loss: 0.0697 - dice_coef: 0.9645 - val_loss: 0.2452 - val_dice_coef: 0.8758\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.21079\n",
      "Epoch 60/100\n",
      "90/90 [==============================] - 207s 2s/step - loss: 0.0687 - dice_coef: 0.9651 - val_loss: 0.2441 - val_dice_coef: 0.8764\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.21079\n",
      "\n",
      "Epoch 00060: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "Epoch 61/100\n",
      "90/90 [==============================] - 206s 2s/step - loss: 0.0685 - dice_coef: 0.9652 - val_loss: 0.2447 - val_dice_coef: 0.8761\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.21079\n",
      "Epoch 62/100\n",
      "45/90 [==============>...............] - ETA: 1:44 - loss: 0.0696 - dice_coef: 0.9646"
     ]
    }
   ],
   "source": [
    "reduce_learning_rate = keras.callbacks.ReduceLROnPlateau(\n",
    "  monitor = \"loss\", \n",
    "  factor = 0.5, \n",
    "  patience = 3, \n",
    "  verbose = 1\n",
    ")\n",
    "\n",
    "checkpointer = keras.callbacks.ModelCheckpoint(\n",
    "  \"unet.h5\", \n",
    "  verbose = 1, \n",
    "  save_best_only = True\n",
    ")\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "if (os.path.exists(\"unet.h5\")):\n",
    "  model = keras.models.load_model(\"unet.h5\",\n",
    "    custom_objects = {\n",
    "      \"jaccard_distance_loss\": jaccard_distance_loss,\n",
    "      \"dice_coef\": dice_coef\n",
    "    }\n",
    "  )\n",
    "  \n",
    "else:\n",
    "  with strategy.scope():\n",
    "    model = unet_model()\n",
    "    adam_opt = keras.optimizers.Adam(learning_rate = 0.001)\n",
    "    model.compile(optimizer = adam_opt, loss = jaccard_distance_loss, metrics = [dice_coef])\n",
    "    \n",
    "  fit = model.fit(train_generator, \n",
    "    steps_per_epoch = steps_per_epoch, \n",
    "    epochs = 100,\n",
    "    validation_data = (X_val, Y_val),\n",
    "    callbacks = [\n",
    "      checkpointer,\n",
    "      reduce_learning_rate\n",
    "    ]\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load('input/X_train.npy')\n",
    "Y_train = np.load('input/Y_train.npy')\n",
    "\n",
    "iou_train, dice_train = model.evaluate(X_train, Y_train, verbose = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iou_val, dice_val = model.evaluate(X_val, Y_val, verbose = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.load('input/X_test.npy')\n",
    "Y_test = np.load('input/Y_test.npy')\n",
    "\n",
    "iou_test, dice_test = model.evaluate(X_test, Y_test, verbose = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Jaccard distance (IoU) train: %f\" % iou_train)\n",
    "print(\"Dice coeffient train: %f\" % dice_train)\n",
    "print(\"Jaccard distance (IoU) validation: %f\" % iou_val)\n",
    "print(\"Dice coeffient validation: %f\" % dice_val)\n",
    "print(\"Jaccard distance (IoU) test: %f\" % iou_test)\n",
    "print(\"Dice coeffient test: %f\" % dice_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nimages = X_train.shape[0]\n",
    "iou_train = []\n",
    "dice_train = []\n",
    "for idx in range(nimages):\n",
    "  iou, dice = model.evaluate(X_train[idx:idx+1,:,:], Y_train[idx:idx+1,:,:], verbose = False)\n",
    "  iou_train.append(iou)\n",
    "  dice_train.append(dice)\n",
    "\n",
    "print(\"Jaccard distance (IoU) train: %f (+-%f)\" % (np.mean(iou_train), np.std(iou_train)))\n",
    "print(\"Dice coeffient train: %f (+-%f)\" % (np.mean(dice_train), np.std(dice_train)))\n",
    "\n",
    "nimages = X_val.shape[0]\n",
    "iou_val = []\n",
    "dice_val = []\n",
    "for idx in range(nimages):\n",
    "  iou, dice = model.evaluate(X_val[idx:idx+1,:,:], Y_val[idx:idx+1,:,:], verbose = False)\n",
    "  iou_val.append(iou)\n",
    "  dice_val.append(dice)\n",
    "\n",
    "print(\"Jaccard distance (IoU) validation: %f (+-%f)\" % (np.mean(iou_val), np.std(iou_val)))\n",
    "print(\"Dice coeffient validation: %f (+-%f)\" % (np.mean(dice_val), np.std(dice_val)))\n",
    "\n",
    "\n",
    "nimages = X_test.shape[0]\n",
    "iou_test = []\n",
    "dice_test = []\n",
    "for idx in range(nimages):\n",
    "  iou, dice = model.evaluate(X_test[idx:idx+1,:,:], Y_test[idx:idx+1,:,:], verbose = False)\n",
    "  iou_test.append(iou)\n",
    "  dice_test.append(dice)\n",
    "\n",
    "print(\"Jaccard distance (IoU) test: %f (+-%f)\" % (np.mean(iou_test), np.std(iou_test)))\n",
    "print(\"Dice coeffient test: %f (+-%f)\" % (np.mean(dice_test), np.std(dice_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nimages = X_test.shape[0]\n",
    "iou_test = []\n",
    "dice_test = []\n",
    "with tf.device(\"/gpu:1\"):\n",
    "  for idx in range(nimages):\n",
    "    iou, dice = model.evaluate(X_test[idx:idx+1,:,:], Y_test[idx:idx+1,:,:], verbose = False)\n",
    "    iou_test.append(iou)\n",
    "    dice_test.append(dice)\n",
    "\n",
    "print(\"Jaccard distance (IoU) test: %f (+-%f)\" % (np.mean(iou_test), np.std(iou_test)))\n",
    "print(\"Dice coeffient test: %f (+-%f)\" % (np.mean(dice_test), np.std(dice_test)))\n",
    "\n",
    "\n",
    "iou_shenzhen = []\n",
    "dice_shenzhen = []\n",
    "with tf.device(\"/gpu:1\"):\n",
    "  for idx in shenzhen_test_ids:\n",
    "    iou, dice = model.evaluate(X_test[idx:idx+1,:,:], Y_test[idx:idx+1,:,:], verbose = False)\n",
    "    iou_shenzhen.append(iou)\n",
    "    dice_shenzhen.append(dice)\n",
    "\n",
    "print(\"Jaccard distance (IoU) Shenzhen: %f (+-%f)\" % (np.mean(iou_shenzhen), np.std(iou_shenzhen)))\n",
    "print(\"Dice coeffient Shenzhen: %f (+-%f)\" % (np.mean(dice_shenzhen), np.std(dice_shenzhen)))\n",
    "\n",
    "\n",
    "iou_montgomery = []\n",
    "dice_montgomery = []\n",
    "with tf.device(\"/gpu:1\"):\n",
    "  for idx in montgomery_test_ids:\n",
    "    iou, dice = model.evaluate(X_test[idx:idx+1,:,:], Y_test[idx:idx+1,:,:], verbose = False)\n",
    "    iou_montgomery.append(iou)\n",
    "    dice_montgomery.append(dice)\n",
    "\n",
    "print(\"Jaccard distance (IoU) Montgomery: %f (+-%f)\" % (np.mean(iou_montgomery), np.std(iou_montgomery)))\n",
    "print(\"Dice coeffient Montgomery: %f (+-%f)\" % (np.mean(dice_montgomery), np.std(dice_montgomery)))\n",
    "\n",
    "\n",
    "iou_jsrt = []\n",
    "dice_jsrt = []\n",
    "with tf.device(\"/gpu:1\"):\n",
    "  for idx in jsrt_test_ids:\n",
    "    iou, dice = model.evaluate(X_test[idx:idx+1,:,:], Y_test[idx:idx+1,:,:], verbose = False)\n",
    "    iou_jsrt.append(iou)\n",
    "    dice_jsrt.append(dice)\n",
    "\n",
    "print(\"Jaccard distance (IoU) JSRT: %f (+-%f)\" % (np.mean(iou_jsrt), np.std(iou_jsrt)))\n",
    "print(\"Dice coeffient JSRT: %f (+-%f)\" % (np.mean(dice_jsrt), np.std(dice_jsrt)))\n",
    "\n",
    "\n",
    "iou_v7labs = []\n",
    "dice_v7labs = []\n",
    "with tf.device(\"/gpu:1\"):\n",
    "  for idx in v7labs_test_ids:\n",
    "    iou, dice = model.evaluate(X_test[idx:idx+1,:,:], Y_test[idx:idx+1,:,:], verbose = False)\n",
    "    iou_v7labs.append(iou)\n",
    "    dice_v7labs.append(dice)\n",
    "\n",
    "print(\"Jaccard distance (IoU) v7labs: %f (+-%f)\" % (np.mean(iou_v7labs), np.std(iou_v7labs)))\n",
    "print(\"Dice coeffient v7labs: %f (+-%f)\" % (np.mean(dice_v7labs), np.std(dice_v7labs)))\n",
    "\n",
    "\n",
    "\n",
    "iou_manual = []\n",
    "dice_manual = []\n",
    "with tf.device(\"/gpu:1\"):\n",
    "  for idx in other_test_ids:\n",
    "    iou, dice = model.evaluate(X_test[idx:idx+1,:,:], Y_test[idx:idx+1,:,:], verbose = False)\n",
    "    iou_manual.append(iou)\n",
    "    dice_manual.append(dice)\n",
    "\n",
    "print(\"Jaccard distance (IoU) manual: %f (+-%f)\" % (np.mean(iou_manual), np.std(iou_manual)))\n",
    "print(\"Dice coeffient manual: %f (+-%f)\" % (np.mean(dice_manual), np.std(dice_manual)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 50\n",
    "test_img = X_test[idx,:,:,:].reshape((1, img_size, img_size, 1))\n",
    "test_mask = Y_test[idx,:,:,:].reshape((1, img_size, img_size, 1))\n",
    "pred_mask = model.predict(test_img)\n",
    "pred_mask = np.uint8(pred_mask > 0.5)\n",
    "post_pred_mask = skimage.morphology.erosion(pred_mask[0,:,:,0], skimage.morphology.square(5))\n",
    "post_pred_mask = skimage.morphology.dilation(post_pred_mask, skimage.morphology.square(10))\n",
    "\n",
    "f = plt.figure(figsize = (20, 10))\n",
    "f.add_subplot(1, 4, 1)\n",
    "plt.imshow(img_as_ubyte(test_img[0,:,:,0]), cmap = \"gray\")\n",
    "f.add_subplot(1, 4, 2)\n",
    "plt.imshow(test_mask[0,:,:,0], cmap = \"gray\")\n",
    "f.add_subplot(1, 4, 3)\n",
    "plt.imshow(pred_mask[0,:,:,0], cmap = \"gray\")\n",
    "f.add_subplot(1, 4, 4)\n",
    "plt.imshow(post_pred_mask, cmap = \"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image(img, mask):\n",
    "  crop_mask = mask > 0\n",
    "  m, n = mask.shape\n",
    "  crop_mask0, crop_mask1 = crop_mask.any(0), crop_mask.any(1)\n",
    "  col_start, col_end = crop_mask0.argmax(), n - crop_mask0[::-1].argmax()\n",
    "  row_start, row_end = crop_mask1.argmax(), m - crop_mask1[::-1].argmax()\n",
    "  return img[row_start:row_end, col_start:col_end], mask[row_start:row_end, col_start:col_end]\n",
    "  \n",
    "#idx = 70\n",
    "idx = 68\n",
    "test_img = X_test[idx,:,:,:].reshape((1, img_size, img_size, 1))\n",
    "test_mask = Y_test[idx,:,:,:].reshape((1, img_size, img_size, 1))\n",
    "pred_mask = model.predict(test_img)[0,:,:,0]\n",
    "pred_mask = np.uint8(pred_mask > 0.5)\n",
    "open_pred_mask = skimage.morphology.erosion(pred_mask, skimage.morphology.square(5))\n",
    "open_pred_mask = skimage.morphology.dilation(open_pred_mask, skimage.morphology.square(5))\n",
    "post_pred_mask = skimage.morphology.dilation(open_pred_mask, skimage.morphology.square(5))\n",
    "\n",
    "crop_img, crop_mask = crop_image(test_img[0,:,:,0], post_pred_mask)\n",
    "\n",
    "crop_img_masked = crop_img * crop_mask\n",
    "\n",
    "f = plt.figure()\n",
    "f.add_subplot(2, 2, 1)\n",
    "plt.imshow(img_as_ubyte(test_img[0,:,:,0]), cmap = \"gray\")\n",
    "f.add_subplot(2, 2, 2)\n",
    "plt.imshow(post_pred_mask, cmap = \"gray\")\n",
    "f.add_subplot(2, 2, 3)\n",
    "plt.imshow(img_as_ubyte(crop_img), cmap = \"gray\")\n",
    "f.add_subplot(2, 2, 4)\n",
    "plt.imshow(crop_mask, cmap = \"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize = (20, 20))\n",
    "f.add_subplot(1, 1, 1)\n",
    "plt.imshow(img_as_ubyte(test_img[0,:,:,0]), cmap = \"gray\")\n",
    "plt.setp(plt.gcf().get_axes(), xticks=[], yticks=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize = (20, 20))\n",
    "f.add_subplot(1, 1, 1)\n",
    "plt.imshow(pred_mask, cmap = \"gray\")\n",
    "plt.setp(plt.gcf().get_axes(), xticks=[], yticks=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize = (20, 20))\n",
    "f.add_subplot(1, 1, 1)\n",
    "plt.imshow(open_pred_mask, cmap = \"gray\")\n",
    "plt.setp(plt.gcf().get_axes(), xticks=[], yticks=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize = (20, 20))\n",
    "f.add_subplot(1, 1, 1)\n",
    "plt.imshow(post_pred_mask, cmap = \"gray\")\n",
    "plt.setp(plt.gcf().get_axes(), xticks=[], yticks=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize = (20, 20))\n",
    "f.add_subplot(1, 1, 1)\n",
    "plt.imshow(crop_img_masked, cmap = \"gray\")\n",
    "plt.setp(plt.gcf().get_axes(), xticks=[], yticks=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize = (20, 20))\n",
    "f.add_subplot(1, 1, 1)\n",
    "plt.imshow(img_as_ubyte(test_img[5,:,:,0]), cmap = \"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_folders = [\"A1\", \"A2\", \"Cohen\", \"Eurorad\", \"Radiopedia\", \"RSNA\", \"Actualmed\", \"Figure1\", \"Kaggle_CRD\"]\n",
    "pathogen_folders = [\"Opacity\", \"COVID-19\", \"Normal\"]\n",
    "dest_folder = \"2_Images_Seg\"\n",
    "\n",
    "if os.path.isdir(dest_folder):\n",
    "  shutil.rmtree(dest_folder)\n",
    "\n",
    "if not os.path.isdir(dest_folder):\n",
    "  os.makedirs(dest_folder)\n",
    "  \n",
    "if not os.path.isdir(os.path.join(dest_folder, \"masks\")):\n",
    "  os.makedirs(os.path.join(dest_folder, \"masks\"))\n",
    "  \n",
    "for target in [\"train\", \"test\"]:\n",
    "  for pathogen in pathogen_folders:\n",
    "    pathogen_folder = os.path.join(dest_folder, target, pathogen)\n",
    "    if not os.path.isdir(pathogen_folder):\n",
    "      os.makedirs(pathogen_folder)\n",
    "\n",
    "with tf.device(\"/gpu:0\"):\n",
    "  targets = [\"train\", \"test\"]\n",
    "  for target in targets:\n",
    "    pathogens = os.listdir(os.path.join(root_folder, target))\n",
    "    for pathogen in pathogens:\n",
    "      images = os.listdir(os.path.join(root_folder, target, pathogen))\n",
    "      for image in images:\n",
    "        source, _, pid, offset, _ = re.split(\"[_.]\", image)\n",
    "        img = imread(os.path.join(root_folder, target, pathogen, image))\n",
    "        img = img_as_float32(img).reshape((1, img_size, img_size, 1))\n",
    "        mask_pred = model.predict(img)[0,:,:,0]\n",
    "        mask_pred = np.uint8(mask_pred > 0.5)\n",
    "        mask_pred = skimage.morphology.erosion(mask_pred, skimage.morphology.square(5))\n",
    "        mask_pred = skimage.morphology.dilation(mask_pred, skimage.morphology.square(10))\n",
    "        img = img_as_ubyte(img[0,:,:,0])\n",
    "\n",
    "        crop_img, crop_mask = crop_image(img, mask_pred)\n",
    "        \n",
    "        mask_filename = \"%s_%s_%s_%s_%s.png\" % (target, source, pathogen, pid, offset)\n",
    "        img_filename = \"%s_%s_%s_%s.png\" % (source, pathogen, pid, offset)\n",
    "\n",
    "        wdt, hgt = crop_img.shape\n",
    "        if wdt < 200 or hgt < 200:\n",
    "          print(img_filename)\n",
    "          #continue\n",
    "\n",
    "        crop_img = crop_img * crop_mask\n",
    "\n",
    "        crop_img = cv2.resize(crop_img, (300, 300), interpolation = cv2.INTER_CUBIC)\n",
    "        crop_mask = cv2.resize(crop_mask, (300, 300), interpolation = cv2.INTER_CUBIC)\n",
    "\n",
    "        imsave(os.path.join(dest_folder, \"masks\", mask_filename), crop_mask * 255)\n",
    "        imsave(os.path.join(dest_folder, target, pathogen, img_filename), crop_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow] *",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
