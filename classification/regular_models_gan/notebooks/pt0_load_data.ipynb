{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "intimate-paris",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpus existem\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ok': True,\n",
       " 'result': {'id': 2058519653,\n",
       "  'is_bot': True,\n",
       "  'first_name': 'cnn_covid',\n",
       "  'username': 'cnn_covid_bot',\n",
       "  'can_join_groups': True,\n",
       "  'can_read_all_group_messages': False,\n",
       "  'supports_inline_queries': False}}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.ndimage\n",
    "import skimage\n",
    "from skimage import io\n",
    "from skimage.segmentation import mark_boundaries\n",
    "from skimage.util import img_as_ubyte\n",
    "\n",
    "import cv2\n",
    "from albumentations import (\n",
    "    Compose, HorizontalFlip, ShiftScaleRotate, ElasticTransform,\n",
    "    RandomBrightness, RandomContrast, RandomGamma, CLAHE\n",
    ")\n",
    "\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import telebot\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import lime\n",
    "from lime import lime_image\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "  try:\n",
    "    print(\"gpus existem\")\n",
    "    print(gpus)\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "  except RuntimeError as e:\n",
    "    print(e)\n",
    "    \n",
    "    \n",
    "TELEBOT_TOKEN = \"2058519653:AAG5Kf0Othtye8e13F5WPnBQQSdoCt47ifA\"\n",
    "\n",
    "bot = telebot.TeleBot(\"2058519653:AAG5Kf0Othtye8e13F5WPnBQQSdoCt47ifA\")\n",
    "bot.config['api_key'] = TELEBOT_TOKEN\n",
    "bot.get_me()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bridal-mistake",
   "metadata": {},
   "source": [
    "## Load Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adapted-scanning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6412\n",
      "1545\n"
     ]
    }
   ],
   "source": [
    "root_folder = \"../../3_Images\"\n",
    "\n",
    "img_size = 300\n",
    "\n",
    "images_train = []\n",
    "labels_train = []\n",
    "\n",
    "for idx, pathogen in enumerate([\"Opacity\", \"COVID-19\", \"Normal\"]):\n",
    "  for img_filename in os.listdir(os.path.join(root_folder, \"train\", pathogen)):\n",
    "    img = cv2.imread(os.path.join(root_folder, \"train\", pathogen, img_filename), 0)\n",
    "    img = cv2.resize(img, (img_size, img_size), interpolation = cv2.INTER_CUBIC)\n",
    "    img = skimage.img_as_float32(img)\n",
    "    images_train.append(img)\n",
    "    labels_train.append(idx)\n",
    "\n",
    "images_test = []\n",
    "labels_test = []\n",
    "for idx, pathogen in enumerate([\"Opacity\", \"COVID-19\", \"Normal\"]):\n",
    "  for img_filename in os.listdir(os.path.join(root_folder, \"test\", pathogen)):\n",
    "    img = cv2.imread(os.path.join(root_folder, \"test\", pathogen, img_filename), 0)\n",
    "    img = cv2.resize(img, (img_size, img_size), interpolation = cv2.INTER_CUBIC)\n",
    "    img = skimage.img_as_float32(img)\n",
    "    images_test.append(img)\n",
    "    labels_test.append(idx)\n",
    "\n",
    "print(len(images_train))\n",
    "print(len(images_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expensive-grill",
   "metadata": {},
   "source": [
    "## Cria Val, Train e Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "swiss-packing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 1, 2]), array([1799, 1645, 1685]))\n",
      "(array([0, 1, 2]), array([443, 414, 426]))\n",
      "(array([0, 1, 2]), array([536, 502, 507]))\n"
     ]
    }
   ],
   "source": [
    "X_train = np.array(images_train).reshape((len(images_train), img_size, img_size))\n",
    "Y_train = keras.utils.to_categorical(labels_train)\n",
    "X_train, Y_train = sklearn.utils.shuffle(X_train, Y_train, random_state = 587)\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.2, random_state = 587)\n",
    "X_val = np.stack((X_val,) * 3, axis = -1)\n",
    "\n",
    "X_test = np.array(images_test).reshape((len(images_test), img_size, img_size))\n",
    "X_test = np.stack((X_test,) * 3, axis = -1)\n",
    "Y_test = keras.utils.to_categorical(labels_test)\n",
    "X_test, Y_test = sklearn.utils.shuffle(X_test, Y_test)\n",
    "\n",
    "print(np.unique(np.argmax(Y_train, axis = 1), return_counts = True))\n",
    "print(np.unique(np.argmax(Y_val, axis = 1), return_counts = True))\n",
    "print(np.unique(np.argmax(Y_test, axis = 1), return_counts = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corporate-lightning",
   "metadata": {},
   "source": [
    "## Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "incredible-penetration",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dell/anaconda3/envs/tensorflow/lib/python3.8/site-packages/albumentations/augmentations/transforms.py:1744: FutureWarning: This class has been deprecated. Please use RandomBrightnessContrast\n",
      "  warnings.warn(\n",
      "/home/dell/anaconda3/envs/tensorflow/lib/python3.8/site-packages/albumentations/augmentations/transforms.py:1770: FutureWarning: This class has been deprecated. Please use RandomBrightnessContrast\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "np.random.seed = 587\n",
    "\n",
    "class AugmentationSequence(keras.utils.Sequence):\n",
    "  def __init__(self, x_set, y_set, batch_size, augmentations):\n",
    "    self.x, self.y = x_set, y_set\n",
    "    self.batch_size = batch_size\n",
    "    self.augment = augmentations\n",
    "\n",
    "  def __len__(self):\n",
    "    return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "    batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "    \n",
    "    aug_x = np.zeros(batch_x.shape)\n",
    "    for idx in range(batch_x.shape[0]):\n",
    "      aug = self.augment(image = batch_x[idx,:,:])\n",
    "      aug_x[idx,:,:] = aug[\"image\"]\n",
    "\n",
    "    return np.stack((aug_x,) * 3, axis = -1), batch_y\n",
    "\n",
    "augment = Compose([\n",
    "  HorizontalFlip(),\n",
    "  ShiftScaleRotate(shift_limit = 0.05, scale_limit = 0.05, rotate_limit = 15, border_mode = cv2.BORDER_CONSTANT),\n",
    "  ElasticTransform(sigma = 20, alpha_affine = 20, border_mode = cv2.BORDER_CONSTANT),\n",
    "  RandomBrightness(),\n",
    "  RandomContrast(),\n",
    "  RandomGamma()\n",
    "])\n",
    "\n",
    "batch_size = 16\n",
    "train_generator = AugmentationSequence(X_train, Y_train, batch_size, augment)\n",
    "steps_per_epoch = math.ceil(X_train.shape[0] / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aggressive-antenna",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "321"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps_per_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generic-patrick",
   "metadata": {},
   "source": [
    "## Salvando datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "residential-portrait",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    " \n",
    "\n",
    "# Step 2\n",
    "with open('input/train_dataset.pkl', 'wb') as file:\n",
    "    pickle.dump(train_generator, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "rising-musician",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_pi2 = open('input/train_dataset.pkl', 'rb') \n",
    "object_pi2 = pickle.load(file_pi2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "absolute-climb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.AugmentationSequence"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "brazilian-automation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.AugmentationSequence"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(object_pi2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dietary-destination",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('input/X_test.npy', X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "subtle-north",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('input/Y_test.npy', Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "joint-pakistan",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('input/X_val.npy', X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "smooth-float",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('input/Y_val.npy', Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "apparent-mouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('input/X_train.npy', X_train)\n",
    "np.save('input/Y_train.npy', Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "adjacent-knight",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('input/X_test.npy')\n",
    "# extract the first array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stylish-diploma",
   "metadata": {},
   "source": [
    "## Load Test 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "floating-arctic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5878\n",
      "2394\n"
     ]
    }
   ],
   "source": [
    "root_folder = \"../../3_Images_Tests\"\n",
    "\n",
    "img_size = 300\n",
    "\n",
    "images_train = []\n",
    "labels_train = []\n",
    "\n",
    "for idx, pathogen in enumerate([\"Opacity\", \"COVID-19\", \"Normal\"]):\n",
    "  for img_filename in os.listdir(os.path.join(root_folder, \"train\", pathogen)):\n",
    "    img = cv2.imread(os.path.join(root_folder, \"train\", pathogen, img_filename), 0)\n",
    "    img = cv2.resize(img, (img_size, img_size), interpolation = cv2.INTER_CUBIC)\n",
    "    img = skimage.img_as_float32(img)\n",
    "    images_train.append(img)\n",
    "    labels_train.append(idx)\n",
    "\n",
    "images_test = []\n",
    "labels_test = []\n",
    "for idx, pathogen in enumerate([\"Opacity\", \"COVID-19\", \"Normal\"]):\n",
    "  for img_filename in os.listdir(os.path.join(root_folder, \"test\", pathogen)):\n",
    "    img = cv2.imread(os.path.join(root_folder, \"test\", pathogen, img_filename), 0)\n",
    "    img = cv2.resize(img, (img_size, img_size), interpolation = cv2.INTER_CUBIC)\n",
    "    img = skimage.img_as_float32(img)\n",
    "    images_test.append(img)\n",
    "    labels_test.append(idx)\n",
    "\n",
    "print(len(images_train))\n",
    "print(len(images_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "african-toilet",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(images_train).reshape((len(images_train), img_size, img_size))\n",
    "Y_train = keras.utils.to_categorical(labels_train)\n",
    "X_train, Y_train = sklearn.utils.shuffle(X_train, Y_train, random_state = 587)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "yellow-invite",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array(images_test).reshape((len(images_test), img_size, img_size))\n",
    "X_test = np.stack((X_test,) * 3, axis = -1)\n",
    "Y_test = keras.utils.to_categorical(labels_test)\n",
    "X_test, Y_test = sklearn.utils.shuffle(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spoken-channel",
   "metadata": {},
   "source": [
    "## Salvando datasets 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "terminal-greek",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('input/X_test2.npy', X_test)\n",
    "np.save('input/Y_test2.npy', Y_test)\n",
    "np.save('input/X_train2.npy', X_train)\n",
    "np.save('input/Y_train2.npy', Y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow] *",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
